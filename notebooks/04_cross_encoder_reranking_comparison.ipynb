{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b09ed61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenAI API key loaded\n",
      "Loading test data...\n",
      "================================================================================\n",
      "CROSS-ENCODER RERANKING COMPARISON (with parallel processing)\n",
      "Test set size: 3080 queries\n",
      "================================================================================\n",
      "\n",
      "[1/3] Testing BASELINE (no reranking)...\n",
      "Initializing baseline retriever...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3080/3080 [01:49<00:00, 28.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Baseline completed in 1.8 minutes\n",
      "\n",
      "[2/3] Testing RERANKING 2x (TinyBERT)...\n",
      "Initializing reranker with 2x over-fetch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3080/3080 [02:04<00:00, 24.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Reranking 2x completed in 2.1 minutes\n",
      "\n",
      "[3/3] Testing RERANKING 5x (TinyBERT)...\n",
      "Initializing reranker with 5x over-fetch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3080/3080 [02:07<00:00, 24.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Reranking 5x completed in 2.1 minutes\n",
      "\n",
      "================================================================================\n",
      "RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Baseline (no reranking): 90.5% (2788/3080) [1.8 min]\n",
      "Reranking 2x (TinyBERT): 91.3% (2812/3080) [+0.8%] [2.1 min]\n",
      "Reranking 5x (TinyBERT): 90.9% (2800/3080) [+0.4%] [2.1 min]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "✓ RECOMMENDATION: Use Reranking 2x\n",
      "  Improvement over baseline: +0.8%\n",
      "  ⚠️ Note: Marginal improvement - consider baseline for simplicity\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PERFORMANCE vs ACCURACY TRADEOFF\n",
      "--------------------------------------------------------------------------------\n",
      "• Baseline (no reranking)  : 90.5% (reference)\n",
      "• Reranking 2x             : 91.3% (+0.8%) | +14% time | Ratio: 0.054\n",
      "• Reranking 5x             : 90.9% (+0.4%) | +17% time | Ratio: 0.023\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "• Both reranking strategies improve over baseline\n",
      "• 2x over-fetch performs better than 5x\n",
      "  → Diminishing returns with too many candidates\n",
      "\n",
      "================================================================================\n",
      "\n",
      "✓ Results saved to: c:\\Users\\victo\\customer-support-rag\\data\\processed\\reranking_comparison.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cross-Encoder Reranking Comparison - Complete evaluation with parallelization\n",
    "Tests: Baseline (no reranking), 2x over-fetch, 5x over-fetch\n",
    "\"\"\"\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "# Find project root\n",
    "current = Path.cwd()\n",
    "while current != current.parent:\n",
    "    if (current / 'src').exists():\n",
    "        project_root = current\n",
    "        break\n",
    "    current = current.parent\n",
    "else:\n",
    "    project_root = Path.cwd().parent\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(project_root / '.env')\n",
    "\n",
    "# Add src to path\n",
    "src_path = str(project_root / 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Force reload the module to get latest changes\n",
    "import importlib\n",
    "if 'retrieval.retriever' in sys.modules:\n",
    "    importlib.reload(sys.modules['retrieval.retriever'])\n",
    "\n",
    "from retrieval.retriever import KnowledgeBaseRetriever\n",
    "\n",
    "# Verify API key\n",
    "import os\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    print(\"ERROR: OPENAI_API_KEY not found!\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print(\"✓ OpenAI API key loaded\")\n",
    "\n",
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "test_df = pd.read_csv(project_root / 'data/processed/test_processed.csv')\n",
    "vector_db_path = str(project_root / 'data/vector_db')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CROSS-ENCODER RERANKING COMPARISON (with parallel processing)\")\n",
    "print(f\"Test set size: {len(test_df)} queries\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def evaluate_single_query(row_tuple, retriever):\n",
    "    \"\"\"Evaluate a single query - designed for parallel execution\"\"\"\n",
    "    idx, row = row_tuple\n",
    "    query = row['text']\n",
    "    true_category = row['category']\n",
    "    \n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = retriever.retrieve(query, n_results=1)\n",
    "            predicted_category = result['metadatas'][0]['category']\n",
    "            return idx, predicted_category == true_category, None\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            # Ignore ChromaDB internal event errors (harmless)\n",
    "            if 'CollectionQueryEvent' in error_msg:\n",
    "                time.sleep(0.1)  # Brief pause\n",
    "                continue\n",
    "            \n",
    "            # For other errors, retry with exponential backoff\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(0.5 * (2 ** attempt))\n",
    "                continue\n",
    "            else:\n",
    "                # Final attempt failed\n",
    "                return idx, False, error_msg\n",
    "    \n",
    "    # If we exhausted retries\n",
    "    return idx, False, \"Max retries exceeded\"\n",
    "\n",
    "def parallel_evaluate(retriever, test_df, max_workers=10):\n",
    "    \"\"\"Evaluate retriever with parallel processing\"\"\"\n",
    "    correct = 0\n",
    "    total = len(test_df)\n",
    "    errors = []\n",
    "    \n",
    "    # Create partial function with retriever bound\n",
    "    eval_func = partial(evaluate_single_query, retriever=retriever)\n",
    "    \n",
    "    # Parallel execution\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        futures = {\n",
    "            executor.submit(eval_func, (idx, row)): idx\n",
    "            for idx, row in test_df.iterrows()\n",
    "        }\n",
    "        \n",
    "        # Process results with progress bar\n",
    "        for future in tqdm(as_completed(futures), total=total, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                idx, is_correct, error = future.result()\n",
    "                if is_correct:\n",
    "                    correct += 1\n",
    "                if error:\n",
    "                    errors.append((idx, error))\n",
    "            except Exception as e:\n",
    "                print(f\"\\nUnexpected error: {e}\")\n",
    "    \n",
    "    # Report errors if any\n",
    "    if errors:\n",
    "        print(f\"\\n⚠️ {len(errors)} queries had errors (counted as incorrect)\")\n",
    "        if len(errors) <= 5:\n",
    "            for idx, error in errors:\n",
    "                print(f\"  Query {idx}: {error[:100]}\")\n",
    "    \n",
    "    return correct, total\n",
    "\n",
    "# Test 1: BASELINE (no reranking)\n",
    "print(\"\\n[1/3] Testing BASELINE (no reranking)...\")\n",
    "print(\"Initializing baseline retriever...\")\n",
    "retriever_baseline = KnowledgeBaseRetriever(\n",
    "    vector_db_path,\n",
    "    use_reranking=False\n",
    ")\n",
    "start_time = time.time()\n",
    "correct_baseline, total_baseline = parallel_evaluate(retriever_baseline, test_df, max_workers=10)\n",
    "baseline_time = time.time() - start_time\n",
    "accuracy_baseline = correct_baseline / total_baseline * 100\n",
    "print(f\"✓ Baseline completed in {baseline_time/60:.1f} minutes\")\n",
    "\n",
    "# Test 2: Reranking with 2x over-fetch\n",
    "print(\"\\n[2/3] Testing RERANKING 2x (TinyBERT)...\")\n",
    "print(\"Initializing reranker with 2x over-fetch...\")\n",
    "retriever_2x = KnowledgeBaseRetriever(\n",
    "    vector_db_path,\n",
    "    use_reranking=True,\n",
    "    rerank_multiplier=2\n",
    ")\n",
    "start_time = time.time()\n",
    "correct_2x, total_2x = parallel_evaluate(retriever_2x, test_df, max_workers=10)\n",
    "rerank_2x_time = time.time() - start_time\n",
    "accuracy_2x = correct_2x / total_2x * 100\n",
    "print(f\"✓ Reranking 2x completed in {rerank_2x_time/60:.1f} minutes\")\n",
    "\n",
    "# Test 3: Reranking with 5x over-fetch\n",
    "print(\"\\n[3/3] Testing RERANKING 5x (TinyBERT)...\")\n",
    "print(\"Initializing reranker with 5x over-fetch...\")\n",
    "retriever_5x = KnowledgeBaseRetriever(\n",
    "    vector_db_path,\n",
    "    use_reranking=True,\n",
    "    rerank_multiplier=5\n",
    ")\n",
    "start_time = time.time()\n",
    "correct_5x, total_5x = parallel_evaluate(retriever_5x, test_df, max_workers=10)\n",
    "rerank_5x_time = time.time() - start_time\n",
    "accuracy_5x = correct_5x / total_5x * 100\n",
    "print(f\"✓ Reranking 5x completed in {rerank_5x_time/60:.1f} minutes\")\n",
    "\n",
    "# Results Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBaseline (no reranking): {accuracy_baseline:.1f}% ({correct_baseline}/{total_baseline}) [{baseline_time/60:.1f} min]\")\n",
    "print(f\"Reranking 2x (TinyBERT): {accuracy_2x:.1f}% ({correct_2x}/{total_2x}) [+{accuracy_2x-accuracy_baseline:.1f}%] [{rerank_2x_time/60:.1f} min]\")\n",
    "print(f\"Reranking 5x (TinyBERT): {accuracy_5x:.1f}% ({correct_5x}/{total_5x}) [+{accuracy_5x-accuracy_baseline:.1f}%] [{rerank_5x_time/60:.1f} min]\")\n",
    "\n",
    "# Determine best approach\n",
    "results = [\n",
    "    (\"Baseline (no reranking)\", accuracy_baseline, baseline_time),\n",
    "    (\"Reranking 2x\", accuracy_2x, rerank_2x_time),\n",
    "    (\"Reranking 5x\", accuracy_5x, rerank_5x_time)\n",
    "]\n",
    "best_name, best_accuracy, best_time = max(results, key=lambda x: x[1])\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "if best_name == \"Baseline (no reranking)\":\n",
    "    print(\"✓ RECOMMENDATION: Use baseline (no reranking)\")\n",
    "    print(\"  The embedding model is already excellent for this dataset!\")\n",
    "    print(\"  Reranking adds complexity without improvement.\")\n",
    "else:\n",
    "    improvement = best_accuracy - accuracy_baseline\n",
    "    print(f\"✓ RECOMMENDATION: Use {best_name}\")\n",
    "    print(f\"  Improvement over baseline: +{improvement:.1f}%\")\n",
    "    if improvement < 1.0:\n",
    "        print(\"  ⚠️ Note: Marginal improvement - consider baseline for simplicity\")\n",
    "    else:\n",
    "        print(\"  ✓ Significant improvement worth the added complexity!\")\n",
    "\n",
    "# Performance/accuracy tradeoff analysis\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"PERFORMANCE vs ACCURACY TRADEOFF\")\n",
    "print(\"-\"*80)\n",
    "for name, acc, exec_time in results:\n",
    "    time_overhead = ((exec_time / baseline_time) - 1) * 100 if name != \"Baseline (no reranking)\" else 0\n",
    "    acc_gain = acc - accuracy_baseline\n",
    "    \n",
    "    if name == \"Baseline (no reranking)\":\n",
    "        print(f\"• {name:25s}: {acc:.1f}% (reference)\")\n",
    "    else:\n",
    "        efficiency_ratio = acc_gain / time_overhead if time_overhead > 0 else 0\n",
    "        print(f\"• {name:25s}: {acc:.1f}% (+{acc_gain:.1f}%) | +{time_overhead:.0f}% time | Ratio: {efficiency_ratio:.3f}\")\n",
    "\n",
    "# Detailed analysis\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"-\"*80)\n",
    "if accuracy_2x > accuracy_baseline and accuracy_5x > accuracy_baseline:\n",
    "    print(\"• Both reranking strategies improve over baseline\")\n",
    "    if accuracy_5x > accuracy_2x + 0.5:\n",
    "        print(\"• 5x over-fetch provides notably better results\")\n",
    "        print(\"  → More candidates allows better reranking\")\n",
    "    elif accuracy_2x > accuracy_5x:\n",
    "        print(\"• 2x over-fetch performs better than 5x\")\n",
    "        print(\"  → Diminishing returns with too many candidates\")\n",
    "    else:\n",
    "        print(\"• Similar performance between 2x and 5x\")\n",
    "        print(\"  → 2x is more efficient with comparable results\")\n",
    "elif accuracy_2x < accuracy_baseline or accuracy_5x < accuracy_baseline:\n",
    "    print(\"• Reranking underperforms baseline\")\n",
    "    print(\"  Possible reasons:\")\n",
    "    print(\"  - Embedding model already captures semantic similarity well\")\n",
    "    print(\"  - Cross-encoder not well-suited for this domain\")\n",
    "    print(\"  - Dataset characteristics favor pure embedding search\")\n",
    "else:\n",
    "    print(\"• Mixed results - some reranking helps, some doesn't\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame({\n",
    "    'Method': ['Baseline', 'Reranking 2x', 'Reranking 5x'],\n",
    "    'Correct': [correct_baseline, correct_2x, correct_5x],\n",
    "    'Total': [total_baseline, total_2x, total_5x],\n",
    "    'Accuracy': [accuracy_baseline, accuracy_2x, accuracy_5x],\n",
    "    'vs_Baseline': [0.0, accuracy_2x - accuracy_baseline, accuracy_5x - accuracy_baseline],\n",
    "    'Time_Minutes': [baseline_time/60, rerank_2x_time/60, rerank_5x_time/60],\n",
    "    'Time_Overhead_Pct': [0.0, ((rerank_2x_time/baseline_time)-1)*100, ((rerank_5x_time/baseline_time)-1)*100]\n",
    "})\n",
    "\n",
    "output_path = project_root / 'data/processed/reranking_comparison.csv'\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"\\n✓ Results saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
