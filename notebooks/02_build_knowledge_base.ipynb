{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943f859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BM25 available for hybrid search\n",
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "# Customer Support RAG - Enhanced Knowledge Base Builder\n",
    "## Building Production-Ready Knowledge Base with OpenAI API\n",
    "\n",
    "This notebook:\n",
    "1. Generates answers for all training queries using GPT-4o-mini\n",
    "2. Creates embeddings using OpenAI text-embedding-3-small\n",
    "3. Stores everything in ChromaDB vector database\n",
    "4. Implements negation handling and evaluation framework\n",
    "5. Adds hybrid retrieval and reranking capabilities\n",
    "6. Properly evaluates on UNSEEN test data\n",
    "7. Exports knowledge base for production use\n",
    "\"\"\"\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup and Imports\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For vector database\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# For hybrid retrieval\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    BM25_AVAILABLE = True\n",
    "    print(\"‚úÖ BM25 available for hybrid search\")\n",
    "except ImportError:\n",
    "    BM25_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  BM25 not available - hybrid search will be disabled\")\n",
    "    print(\"   Install with: pip install rank-bm25\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f78133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\victo\\customer-support-rag\n",
      "Looking for .env at: c:\\Users\\victo\\customer-support-rag\\.env\n",
      "Exists: True\n",
      "\n",
      "‚úÖ OpenAI client initialized\n",
      "   API key: sk-proj-YNmq1tao-Q91...0g08A\n",
      "‚úÖ Connected to OpenAI API\n",
      "   Available models: 111\n",
      "\n",
      "======================================================================\n",
      "Setup complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 1. Setup paths and environment\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "env_path = project_root / '.env'\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Looking for .env at: {env_path}\")\n",
    "print(f\"Exists: {env_path.exists()}\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "# Initialize OpenAI client\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if api_key:\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    print(f\"\\n‚úÖ OpenAI client initialized\")\n",
    "    print(f\"   API key: {api_key[:20]}...{api_key[-5:]}\")\n",
    "    \n",
    "    # Test connection\n",
    "    try:\n",
    "        models = client.models.list()\n",
    "        print(f\"‚úÖ Connected to OpenAI API\")\n",
    "        print(f\"   Available models: {len(models.data)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Connection failed: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå OPENAI_API_KEY not found!\")\n",
    "    print(\"Make sure .env file contains: OPENAI_API_KEY=sk-...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Setup complete!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "479dc46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data for knowledge base...\n",
      "‚ö†Ô∏è  Note: Test data will be loaded SEPARATELY later for evaluation\n",
      "\n",
      "‚úÖ Training set: 10,003 queries (for knowledge base)\n",
      "‚úÖ Categories: 77\n",
      "\n",
      "Training data info:\n",
      "   Columns: ['text', 'label', 'text_length', 'word_count', 'char_count', 'category', 'question_type', 'has_and', 'has_or', 'has_but', 'has_negation', 'has_multiple_sentences', 'has_question_mark', 'is_complex']\n",
      "   Avg query length: 11.9 words\n",
      "   Categories: 77\n",
      "   Queries with negation: 2184 (21.8%)\n",
      "   Complex queries: 1815 (18.1%)\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>word_count</th>\n",
       "      <th>has_negation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am still waiting on my card?</td>\n",
       "      <td>card_arrival</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What can I do if my card still hasn't arrived ...</td>\n",
       "      <td>card_arrival</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have been waiting over a week. Is the card s...</td>\n",
       "      <td>card_arrival</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can I track my card while it is in the process...</td>\n",
       "      <td>card_arrival</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do I know if I will get my card, or if it ...</td>\n",
       "      <td>card_arrival</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      category  \\\n",
       "0                     I am still waiting on my card?  card_arrival   \n",
       "1  What can I do if my card still hasn't arrived ...  card_arrival   \n",
       "2  I have been waiting over a week. Is the card s...  card_arrival   \n",
       "3  Can I track my card while it is in the process...  card_arrival   \n",
       "4  How do I know if I will get my card, or if it ...  card_arrival   \n",
       "\n",
       "   word_count  has_negation  \n",
       "0           7         False  \n",
       "1          13          True  \n",
       "2          12         False  \n",
       "3          13         False  \n",
       "4          15         False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## 2. Load Training Data (ONLY - For Knowledge Base)\n",
    "\n",
    "# %%\n",
    "print(\"Loading training data for knowledge base...\")\n",
    "print(\"‚ö†Ô∏è  Note: Test data will be loaded SEPARATELY later for evaluation\\n\")\n",
    "\n",
    "# Load ONLY training data for knowledge base\n",
    "train_df = pd.read_csv('../data/processed/train_processed.csv')\n",
    "\n",
    "# Load category mapping\n",
    "with open('../data/processed/category_mapping.json', 'r') as f:\n",
    "    category_mapping = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Training set: {len(train_df):,} queries (for knowledge base)\")\n",
    "print(f\"‚úÖ Categories: {len(category_mapping)}\")\n",
    "\n",
    "print(f\"\\nTraining data info:\")\n",
    "print(f\"   Columns: {list(train_df.columns)}\")\n",
    "print(f\"   Avg query length: {train_df['word_count'].mean():.1f} words\")\n",
    "print(f\"   Categories: {train_df['category'].nunique()}\")\n",
    "print(f\"   Queries with negation: {train_df['has_negation'].sum()} ({train_df['has_negation'].sum()/len(train_df)*100:.1f}%)\")\n",
    "print(f\"   Complex queries: {train_df['is_complex'].sum()} ({train_df['is_complex'].sum()/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nSample data:\")\n",
    "display(train_df[['text', 'category', 'word_count', 'has_negation']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbea5355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ PRODUCTION MODE\n",
      "======================================================================\n",
      "Using: 10,003 queries from TRAINING SET\n",
      "Coverage: ALL 77 categories (100%)\n",
      "Estimated cost: ~$2.00\n",
      "Estimated time: ~60 minutes\n",
      "‚úÖ Hybrid search: ENABLED (sufficient data)\n",
      "\n",
      "This will create production-ready knowledge base\n",
      "======================================================================\n",
      "\n",
      "Top 10 categories in dataset:\n",
      "category\n",
      "card_payment_fee_charged                            187\n",
      "direct_debit_payment_not_recognised                 182\n",
      "balance_not_updated_after_cheque_or_cash_deposit    181\n",
      "wrong_amount_of_cash_received                       180\n",
      "cash_withdrawal_charge                              177\n",
      "transaction_charged_twice                           175\n",
      "declined_cash_withdrawal                            173\n",
      "transfer_fee_charged                                172\n",
      "transfer_not_received_by_recipient                  171\n",
      "balance_not_updated_after_bank_transfer             171\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample negation queries:\n",
      "  - What can I do if my card still hasn't arrived after 2 weeks?\n",
      "  - What do I do if I still have not received my new card?\n",
      "  - Why has my new card still not come?\n"
     ]
    }
   ],
   "source": [
    "# ## 3. Enhanced Sampling Strategy\n",
    "\n",
    "# %%\n",
    "def create_stratified_sample(df, sample_size=100, min_per_category=1, random_state=42):\n",
    "    \"\"\"\n",
    "    Create stratified sample ensuring minimum representation per category\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to sample from\n",
    "        sample_size: Total samples to return\n",
    "        min_per_category: Minimum samples per category\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Stratified sample DataFrame\n",
    "    \"\"\"\n",
    "    category_counts = df['category'].value_counts()\n",
    "    n_categories = len(category_counts)\n",
    "    \n",
    "    # Allocate samples\n",
    "    samples_per_category = {}\n",
    "    remaining = sample_size - (n_categories * min_per_category)\n",
    "    \n",
    "    if remaining < 0:\n",
    "        print(f\"‚ö†Ô∏è  Warning: sample_size too small for {n_categories} categories\")\n",
    "        remaining = 0\n",
    "        min_per_category = sample_size // n_categories\n",
    "    \n",
    "    for cat, count in category_counts.items():\n",
    "        base = min_per_category\n",
    "        # Allocate remaining proportionally\n",
    "        additional = int(remaining * (count / len(df)))\n",
    "        samples_per_category[cat] = min(base + additional, count)\n",
    "    \n",
    "    # Sample from each category\n",
    "    sampled_dfs = []\n",
    "    for cat, n in samples_per_category.items():\n",
    "        cat_df = df[df['category'] == cat]\n",
    "        n_actual = min(n, len(cat_df))\n",
    "        sampled_dfs.append(cat_df.sample(n=n_actual, random_state=random_state))\n",
    "    \n",
    "    result = pd.concat(sampled_dfs).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"‚úÖ Stratified sampling complete:\")\n",
    "    print(f\"   Target: {sample_size} samples\")\n",
    "    print(f\"   Actual: {len(result)} samples\")\n",
    "    print(f\"   Categories covered: {result['category'].nunique()}/{n_categories}\")\n",
    "    print(f\"   Min per category: {result['category'].value_counts().min()}\")\n",
    "    print(f\"   Max per category: {result['category'].value_counts().max()}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# %%\n",
    "\"\"\"\n",
    "Choose between testing mode (100 queries) or production mode (full dataset)\n",
    "\"\"\"\n",
    "\n",
    "# CONFIGURATION - CHANGE THESE SETTINGS\n",
    "USE_SAMPLE = False  # Set to False for full dataset\n",
    "SAMPLE_SIZE = 100  # Only used if USE_SAMPLE = True\n",
    "USE_STRATIFIED_SAMPLING = True  # Use enhanced sampling strategy\n",
    "\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    # Choose sampling strategy\n",
    "    if USE_STRATIFIED_SAMPLING:\n",
    "        kb_df = create_stratified_sample(train_df, sample_size=SAMPLE_SIZE, random_state=42)\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üß™ TESTING MODE - STRATIFIED SAMPLING\")\n",
    "        print(\"=\"*70)\n",
    "    else:\n",
    "        kb_df = train_df.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üß™ TESTING MODE - RANDOM SAMPLING\")\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    print(f\"Using: {len(kb_df)} queries from TRAINING SET\")\n",
    "    print(f\"Coverage: {kb_df['category'].nunique()}/77 categories ({kb_df['category'].nunique()/77*100:.1f}%)\")\n",
    "    print(f\"Negation queries: {kb_df['has_negation'].sum()} ({kb_df['has_negation'].sum()/len(kb_df)*100:.1f}%)\")\n",
    "    print(f\"Complex queries: {kb_df['is_complex'].sum()} ({kb_df['is_complex'].sum()/len(kb_df)*100:.1f}%)\")\n",
    "    print(f\"Estimated cost: ~$0.30\")\n",
    "    print(f\"Estimated time: ~5 minutes\")\n",
    "    print(f\"‚ö†Ô∏è Hybrid search: DISABLED (insufficient data)\")\n",
    "    print(\"\\nThis is for TESTING only!\")\n",
    "    print(\"Set USE_SAMPLE = False for production dataset\")\n",
    "\n",
    "    USE_HYBRID_SEARCH = False  # Disable for small datasets\n",
    "    \n",
    "else:\n",
    "    # Production mode - full dataset\n",
    "    kb_df = train_df.copy()\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"üöÄ PRODUCTION MODE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Using: {len(kb_df):,} queries from TRAINING SET\")\n",
    "    print(f\"Coverage: ALL 77 categories (100%)\")\n",
    "    print(f\"Estimated cost: ~$2.00\")\n",
    "    print(f\"Estimated time: ~60 minutes\")\n",
    "    print(f\"‚úÖ Hybrid search: ENABLED (sufficient data)\")\n",
    "    print(\"\\nThis will create production-ready knowledge base\")\n",
    "\n",
    "    USE_HYBRID_SEARCH = True  # Enable for large datasets\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show category distribution\n",
    "print(f\"\\nTop 10 categories in dataset:\")\n",
    "print(kb_df['category'].value_counts().head(10))\n",
    "\n",
    "# Show negation examples\n",
    "print(f\"\\nSample negation queries:\")\n",
    "negation_samples = kb_df[kb_df['has_negation'] == True].head(3)\n",
    "for idx, row in negation_samples.iterrows():\n",
    "    print(f\"  - {row['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "164b94a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 16 answer templates for cost optimization\n",
      "   Categories covered: card_arrival, activate_my_card, card_delivery_estimate, change_pin, pin_blocked...\n",
      "   Expected template usage: ~20.8% of queries\n",
      "   Cost savings: ~30-40% on answer generation\n",
      "‚úÖ Enhanced answer generation function defined\n",
      "Testing answer generation with sample queries...\n",
      "\n",
      "Q: I have received my statement but I do not see my refund, why is that?\n",
      "Category: Refund_not_showing_up\n",
      "Has negation: Yes ‚ö†Ô∏è\n",
      "A: I'm sorry to hear that your refund isn't showing up on your statement. Refunds can take 3-5 business days to process and appear in your account, depending on the merchant's processing time. Please double-check that the refund was initiated by the merchant, and if it's still not visible after that timeframe, feel free to reach out to customer support for further assistance.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Q: I was mugged.  They took everything.  I can't use the app.  What do I do?\n",
      "Category: lost_or_stolen_phone\n",
      "Has negation: Yes ‚ö†Ô∏è\n",
      "A: I‚Äôm so sorry to hear about your situation; that must be incredibly distressing. Please contact us immediately to report your phone as lost or stolen so we can secure your account and prevent any unauthorized transactions. You can reach us at [customer service number] or through our website. Additionally, if you have access to another device, you can log into your account on our website to manage your settings until you get your phone back.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Q: I want to apply for a Visa card.\n",
      "Category: visa_or_mastercard\n",
      "Has negation: No\n",
      "A: To apply for a Visa card, you can visit our website or open our mobile app and navigate to the \"Cards\" section. There, you will find an option to apply for a Visa card. Please ensure you have your personal information and identification ready, as the application process typically takes 5-7 business days for approval. If you have any questions during the application, feel free to reach out for assistance!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Q: I am still waiting for a transfer to show\n",
      "Category: pending_transfer\n",
      "Has negation: No\n",
      "A: I understand how frustrating it can be to wait for a transfer to show up. Transfers typically take 1-3 business days to process, depending on the banks involved and the type of transfer. Please check your transaction history again, and if it hasn't appeared after 3 business days, feel free to contact us for further assistance.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ## 4. Enhanced Answer Generation with Templates\n",
    "\n",
    "# %%\n",
    "# Answer templates for common categories\n",
    "ANSWER_TEMPLATES = {\n",
    "    # Card arrival & activation (common)\n",
    "    \"card_arrival\": \"\"\"Your card typically arrives within 5-7 business days after ordering. \n",
    "    If it's been longer than this, please check your delivery address in the app under Settings > Card Details. \n",
    "    If the address is correct and it's been over 10 business days, contact our support team and we'll investigate or send a replacement.\"\"\",\n",
    "    \n",
    "    \"activate_my_card\": \"\"\"To activate your card, open the app and navigate to Cards > Activate Card.\n",
    "    You'll need to enter the last 4 digits of your card number and the CVV code on the back. \n",
    "    Activation is instant and you can start using your card immediately.\"\"\",\n",
    "    \n",
    "    \"card_delivery_estimate\": \"\"\"Standard card delivery takes 5-7 business days. \n",
    "    Express delivery (if selected) takes 2-3 business days. \n",
    "    You'll receive a tracking notification when your card ships. Delivery times may be longer during holidays or to remote areas.\"\"\",\n",
    "    \n",
    "    # PIN management (very common)\n",
    "    \"change_pin\": \"\"\"You can change your PIN anytime through the app. Go to Settings > Security > Change PIN. \n",
    "    You'll need to enter your current PIN, then choose your new 4-digit PIN. \n",
    "    For security, avoid using easily guessed numbers like 1234 or your birth year.\"\"\",\n",
    "    \n",
    "    \"pin_blocked\": \"\"\"If you've entered your PIN incorrectly multiple times, your card is temporarily blocked for security. \n",
    "    To unblock it, open the app and go to Cards > Unblock PIN. \n",
    "    You'll need to verify your identity. If you've forgotten your PIN, you can reset it in Settings > Security > Reset PIN.\"\"\",\n",
    "    \n",
    "    # Payment issues (very common)\n",
    "    \"declined_card_payment\": \"\"\"Card payments can be declined for several reasons: insufficient funds, exceeded spending limits, expired card, incorrect PIN, or security holds. \n",
    "    Check your account balance and card status in the app. \n",
    "    If everything looks correct, contact support as there may be a security flag on your account.\"\"\",\n",
    "    \n",
    "    \"card_payment_not_recognised\": \"\"\"If you see an unrecognized payment, first check if it's a merchant with a different trading name than the store name. \n",
    "    Check your recent transactions for the exact amount and date. \n",
    "    If you still don't recognize it, report it immediately through the app under Transactions > Dispute Payment, and we'll investigate within 1-3 business days.\"\"\",\n",
    "    \n",
    "    \"card_payment_fee_charged\": \"\"\"Payment fees can occur for several reasons: international transactions (typically 2-3%), ATM withdrawals outside our network, exceeding monthly transaction limits, or merchant processing fees. \n",
    "    Check your transaction details in the app for the specific fee breakdown. \n",
    "    Our standard fees are listed in Settings > Fees & Limits.\"\"\",\n",
    "    \n",
    "    # Balance & transfers (common)\n",
    "    \"balance_not_updated_after_bank_transfer\": \"\"\"Bank transfers typically take 1-3 business days to process and appear in your balance. If you initiated the transfer on a weekend or holiday, add an extra day. Check your transaction history for a pending status. If it's been over 3 business days, contact support with your transfer reference number.\"\"\",\n",
    "    \n",
    "    \"balance_not_updated_after_cheque_or_cash_deposit\": \"\"\"Cheque deposits take 2-5 business days to clear and appear in your available balance. \n",
    "    Cash deposits at supported locations usually appear within 24 hours. \n",
    "    Check your transaction history for pending deposits. If it's been longer than expected, contact support with your deposit receipt.\"\"\",\n",
    "    \n",
    "    # Top-up issues (common)\n",
    "    \"top_up_failed\": \"\"\"Top-up failures usually occur due to insufficient funds in your source account, incorrect card details, or temporary banking issues. \n",
    "    Verify your payment method in Settings > Payment Methods and try again. \n",
    "    If it continues failing, try a different payment method or contact support.\"\"\",\n",
    "    \n",
    "    \"pending_top_up\": \"\"\"Top-ups are usually instant but can take up to 30 minutes during high-traffic periods. \n",
    "    Check your transaction history for the pending status. \n",
    "    If it's been over 1 hour, contact support with your transaction reference number and we'll investigate immediately.\"\"\",\n",
    "    \n",
    "    # Virtual card (common)\n",
    "    \"getting_virtual_card\": \"\"\"Virtual cards are issued instantly upon account approval. \n",
    "    Open the app and go to Cards > Add Virtual Card. \n",
    "    If you don't see this option, your account may need verification first. \n",
    "    Complete any pending identity verification in Settings > Account, then try again.\"\"\",\n",
    "    \n",
    "    # Direct debit (common)\n",
    "    \"direct_debit_payment_not_recognised\": \"\"\"If you see an unexpected direct debit, check if it's a subscription or recurring payment you set up. \n",
    "    Common ones include: streaming services, gym memberships, or utility bills. \n",
    "    Check Transactions > Recurring for your active direct debits. \n",
    "    If you don't recognize it, you can dispute it within 60 days.\"\"\",\n",
    "    \n",
    "    # Refunds (common)\n",
    "    \"request_refund\": \"\"\"To request a refund, go to Transactions, select the payment, and tap Request Refund. \n",
    "    You'll need to provide a reason. Merchant refunds typically take 5-10 business days to process. \n",
    "    If the merchant approves, you'll see it in your account. \n",
    "    If denied, you can escalate to our disputes team.\"\"\",\n",
    "    \n",
    "    # Country support (common)\n",
    "    \"country_support\": \"\"\"We currently support accounts in 30+ countries across Europe, North America, and parts of Asia. \n",
    "    To check if your country is supported, visit our website or check Settings > Supported Countries in the app. \n",
    "    Some features may be limited in certain regions due to local regulations.\"\"\",\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(ANSWER_TEMPLATES)} answer templates for cost optimization\")\n",
    "print(f\"   Categories covered: {', '.join(list(ANSWER_TEMPLATES.keys())[:5])}...\")\n",
    "print(f\"   Expected template usage: ~{len(ANSWER_TEMPLATES)/77*100:.1f}% of queries\")\n",
    "print(f\"   Cost savings: ~30-40% on answer generation\")\n",
    "\n",
    "\n",
    "def generate_answer_openai(\n",
    "    question: str,\n",
    "    category: str,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    max_retries: int = 3,\n",
    "    use_template: bool = True\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a helpful answer for a banking customer support question\n",
    "    \n",
    "    Args:\n",
    "        question: Customer's question\n",
    "        category: Question category\n",
    "        model: OpenAI model to use\n",
    "        max_retries: Number of retry attempts\n",
    "        use_template: Whether to use templates for common categories\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer string\n",
    "    \"\"\"\n",
    "    # Check if template exists for this category\n",
    "    if use_template and category in ANSWER_TEMPLATES:\n",
    "        return ANSWER_TEMPLATES[category]\n",
    "    \n",
    "    # Clean category name for display\n",
    "    category_clean = category.replace('_', ' ').title()\n",
    "    \n",
    "    # Enhanced system prompt with negation awareness\n",
    "    system_prompt = \"\"\"You are a helpful customer support agent for a digital bank.\n",
    "Provide clear, concise, and helpful answers to customer questions.\n",
    "\n",
    "Guidelines:\n",
    "- Be professional but friendly\n",
    "- Keep answers to 2-4 sentences\n",
    "- Provide specific steps if applicable\n",
    "- Use realistic timeframes (e.g., \"1-3 business days\", \"5-7 business days\")\n",
    "- If mentioning fees/limits, use typical banking ranges\n",
    "- Don't make up features that don't exist in a typical banking app\n",
    "- Pay special attention to negations (not, didn't, hasn't, never)\n",
    "- For negative situations (card not working, payment not received), acknowledge the issue and provide troubleshooting steps\"\"\"\n",
    "\n",
    "    # User prompt\n",
    "    user_prompt = f\"\"\"Customer Question: {question}\n",
    "Category: {category_clean}\n",
    "\n",
    "Provide a helpful answer:\"\"\"\n",
    "    \n",
    "    # Retry logic for API reliability\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                max_tokens=200,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content.strip()\n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt  # Exponential backoff\n",
    "                print(f\"   ‚ö†Ô∏è  Retry {attempt + 1}/{max_retries} after {wait_time}s: {e}\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"   ‚ùå Failed after {max_retries} attempts: {e}\")\n",
    "                return f\"I understand you're asking about {category_clean}. Please contact our support team for immediate assistance.\"\n",
    "\n",
    "print(\"‚úÖ Enhanced answer generation function defined\")\n",
    "\n",
    "# %%\n",
    "# Test answer generation with sample queries including negations\n",
    "print(\"Testing answer generation with sample queries...\\n\")\n",
    "\n",
    "test_samples = []\n",
    "# Get some negation examples\n",
    "negation_samples = kb_df[kb_df['has_negation'] == True].sample(min(2, kb_df['has_negation'].sum()))\n",
    "# Get some regular examples\n",
    "regular_samples = kb_df[kb_df['has_negation'] == False].sample(min(2, (~kb_df['has_negation']).sum()))\n",
    "test_samples = pd.concat([negation_samples, regular_samples])\n",
    "\n",
    "for idx, row in test_samples.iterrows():\n",
    "    question = row['text']\n",
    "    category = row['category']\n",
    "    has_negation = row['has_negation']\n",
    "    \n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"Category: {category}\")\n",
    "    print(f\"Has negation: {'Yes ‚ö†Ô∏è' if has_negation else 'No'}\")\n",
    "    \n",
    "    answer = generate_answer_openai(question, category)\n",
    "    \n",
    "    print(f\"A: {answer}\\n\")\n",
    "    print(\"-\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3972698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting answer generation...\n",
      "======================================================================\n",
      "Generating answers for 10003 queries...\n",
      "Model: gpt-4o-mini\n",
      "Using templates: True\n",
      " Parallel workers: 10\n",
      "Estimated time: 33.3 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  11%|‚ñà         | 1110/10003 [02:49<01:36, 92.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 1000/10003 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  21%|‚ñà‚ñà        | 2069/10003 [05:16<00:41, 190.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 2000/10003 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  31%|‚ñà‚ñà‚ñà       | 3090/10003 [07:56<00:56, 123.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 3000/10003 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  40%|‚ñà‚ñà‚ñà‚ñâ      | 3996/10003 [09:56<21:09,  4.73it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 4000/10003 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 4998/10003 [12:38<22:00,  3.79it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 5000/10003 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 5981/10003 [15:00<00:23, 172.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 6000/10003 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 6991/10003 [18:24<00:26, 114.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 7000/10003 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 7997/10003 [22:09<09:29,  3.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 8000/10003 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 8999/10003 [24:34<02:57,  5.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 9000/10003 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 9991/10003 [27:26<00:00, 126.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Checkpoint saved: 10000/10003 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10003/10003 [27:28<00:00,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Answer generation complete!\n",
      "   Total: 10003\n",
      "   From templates: 2393 (23.9%)\n",
      "   Generated: 7610\n",
      "   Failed: 0\n",
      "\n",
      "======================================================================\n",
      "‚úÖ All answers generated!\n",
      "======================================================================\n",
      "\n",
      "Answer Statistics:\n",
      "   Avg answer length: 60.7 words\n",
      "   Min answer length: 28 words\n",
      "   Max answer length: 147 words\n",
      "\n",
      "Sample Q&A pairs:\n",
      "\n",
      "1.\n",
      "Q: I am still waiting on my card?\n",
      "Category: card_arrival\n",
      "A: Your card typically arrives within 5-7 business days after ordering. \n",
      "    If it's been longer than this, please check your delivery address in the app under Settings > Card Details. \n",
      "    If the address is correct and it's been over 10 business days, contact our support team and we'll investigate or send a replacement.\n",
      "\n",
      "2.\n",
      "Q: What can I do if my card still hasn't arrived after 2 weeks?\n",
      "Category: card_arrival\n",
      "‚ö†Ô∏è  Contains negation\n",
      "A: Your card typically arrives within 5-7 business days after ordering. \n",
      "    If it's been longer than this, please check your delivery address in the app under Settings > Card Details. \n",
      "    If the address is correct and it's been over 10 business days, contact our support team and we'll investigate or send a replacement.\n",
      "\n",
      "3.\n",
      "Q: I have been waiting over a week. Is the card still coming?\n",
      "Category: card_arrival\n",
      "A: Your card typically arrives within 5-7 business days after ordering. \n",
      "    If it's been longer than this, please check your delivery address in the app under Settings > Card Details. \n",
      "    If the address is correct and it's been over 10 business days, contact our support team and we'll investigate or send a replacement.\n",
      "\n",
      "4.\n",
      "Q: Can I track my card while it is in the process of delivery?\n",
      "Category: card_arrival\n",
      "A: Your card typically arrives within 5-7 business days after ordering. \n",
      "    If it's been longer than this, please check your delivery address in the app under Settings > Card Details. \n",
      "    If the address is correct and it's been over 10 business days, contact our support team and we'll investigate or send a replacement.\n",
      "\n",
      "5.\n",
      "Q: How do I know if I will get my card, or if it is lost?\n",
      "Category: card_arrival\n",
      "A: Your card typically arrives within 5-7 business days after ordering. \n",
      "    If it's been longer than this, please check your delivery address in the app under Settings > Card Details. \n",
      "    If the address is correct and it's been over 10 business days, contact our support team and we'll investigate or send a replacement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ## 5. Generate Answers for All Queries\n",
    "\n",
    "# %%\n",
    "def generate_answers_batch(\n",
    "    df: pd.DataFrame,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    batch_size: int = 20,  \n",
    "    save_progress: bool = True,\n",
    "    use_templates: bool = True,\n",
    "    max_workers: int = 10  \n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Generate answers for all queries with PARALLEL processing\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'text' and 'category' columns\n",
    "        model: OpenAI model to use\n",
    "        batch_size: DEPRECATED - kept for compatibility, not used\n",
    "        save_progress: Save checkpoints every 1000 queries\n",
    "        use_templates: Use answer templates for common categories\n",
    "        max_workers: Number of parallel API calls (5-20 recommended)\n",
    "    \n",
    "    Returns:\n",
    "        List of generated answers\n",
    "    \"\"\"\n",
    "    answers = [None] * len(df)  \n",
    "    failed_indices = []\n",
    "    template_count = 0\n",
    "    \n",
    "    print(f\"Generating answers for {len(df)} queries...\")\n",
    "    print(f\"Model: {model}\")\n",
    "    print(f\"Using templates: {use_templates}\")\n",
    "    print(f\" Parallel workers: {max_workers}\") \n",
    "    print(f\"Estimated time: {len(df) * 2 / 60 / max_workers:.1f} minutes\\n\") \n",
    "    \n",
    "    # üÜï NEW - Define function to process single row\n",
    "    def process_single_query(idx_row_tuple):\n",
    "        \"\"\"Process a single query - designed for parallel execution\"\"\"\n",
    "        idx, row = idx_row_tuple\n",
    "        question = row['text']\n",
    "        category = row['category']\n",
    "        \n",
    "        try:\n",
    "            answer = generate_answer_openai(\n",
    "                question, \n",
    "                category, \n",
    "                model=model, \n",
    "                use_template=use_templates\n",
    "            )\n",
    "            \n",
    "            # Track template usage\n",
    "            used_template = use_templates and category in ANSWER_TEMPLATES\n",
    "            \n",
    "            return idx, answer, used_template, None  # idx, answer, template_flag, error\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Please contact support regarding {category.replace('_', ' ')}\"\n",
    "            return idx, error_msg, False, str(e)\n",
    "    \n",
    "    # üÜï NEW - Parallel execution with ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        futures = {\n",
    "            executor.submit(process_single_query, (idx, row)): idx \n",
    "            for idx, row in df.iterrows()\n",
    "        }\n",
    "        \n",
    "        # Process completed tasks with progress bar\n",
    "        for future in tqdm(\n",
    "            concurrent.futures.as_completed(futures),\n",
    "            total=len(futures),\n",
    "            desc=\"Generating answers\"\n",
    "        ):\n",
    "            try:\n",
    "                idx, answer, used_template, error = future.result()\n",
    "                answers[idx] = answer\n",
    "                \n",
    "                if used_template:\n",
    "                    template_count += 1\n",
    "                \n",
    "                if error:\n",
    "                    failed_indices.append(idx)\n",
    "                    print(f\"\\n‚ùå Failed for index {idx}: {error}\")\n",
    "                \n",
    "                # Save progress every 1000 queries\n",
    "                if save_progress and (idx + 1) % 1000 == 0:\n",
    "                    temp_df = df.iloc[:idx+1].copy()\n",
    "                    temp_df['answer'] = answers[:idx+1]\n",
    "                    checkpoint_path = project_root / 'data' / 'processed' / f'kb_checkpoint_{idx+1}.csv'\n",
    "                    temp_df.to_csv(checkpoint_path, index=False)\n",
    "                    print(f\"\\nüíæ Checkpoint saved: {idx + 1}/{len(df)} completed\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                idx = futures[future]\n",
    "                print(f\"\\n‚ùå Unexpected error for index {idx}: {e}\")\n",
    "                failed_indices.append(idx)\n",
    "                answers[idx] = \"Error generating answer\"\n",
    "    \n",
    "    print(f\"\\n‚úÖ Answer generation complete!\")\n",
    "    print(f\"   Total: {len(answers)}\")\n",
    "    print(f\"   From templates: {template_count} ({template_count/len(answers)*100:.1f}%)\")\n",
    "    print(f\"   Generated: {len(answers) - template_count}\")\n",
    "    print(f\"   Failed: {len(failed_indices)}\")\n",
    "    if failed_indices:\n",
    "        print(f\"   Failed indices: {failed_indices[:10]}...\")\n",
    "    \n",
    "    return answers\n",
    "\n",
    "# %%\n",
    "# Generate answers for all queries in dataset\n",
    "print(\"Starting answer generation...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# üÜï OPTIONAL - Adjust max_workers based on your needs\n",
    "# max_workers=5  : Conservative, ~40 min for 10k queries\n",
    "# max_workers=10 : Balanced (recommended), ~20 min for 10k queries  \n",
    "# max_workers=20 : Aggressive, ~12 min for 10k queries (may hit rate limits)\n",
    "\n",
    "answers = generate_answers_batch(\n",
    "    kb_df, \n",
    "    model=\"gpt-4o-mini\", \n",
    "    use_templates=True,\n",
    "    max_workers=10  \n",
    ")\n",
    "\n",
    "# Add answers to dataframe\n",
    "kb_df['answer'] = answers\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ All answers generated!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Show statistics\n",
    "print(f\"\\nAnswer Statistics:\")\n",
    "kb_df['answer_length'] = kb_df['answer'].str.split().str.len()\n",
    "print(f\"   Avg answer length: {kb_df['answer_length'].mean():.1f} words\")\n",
    "print(f\"   Min answer length: {kb_df['answer_length'].min()} words\")\n",
    "print(f\"   Max answer length: {kb_df['answer_length'].max()} words\")\n",
    "\n",
    "# Show sample Q&A pairs including negations\n",
    "print(f\"\\nSample Q&A pairs:\")\n",
    "for i in range(min(5, len(kb_df))):\n",
    "    row = kb_df.iloc[i]\n",
    "    print(f\"\\n{i+1}.\")\n",
    "    print(f\"Q: {row['text']}\")\n",
    "    print(f\"Category: {row['category']}\")\n",
    "    if row['has_negation']:\n",
    "        print(f\"‚ö†Ô∏è  Contains negation\")\n",
    "    print(f\"A: {row['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cef87b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing texts for embedding...\n",
      "‚úÖ Combined 10003 question-answer pairs\n",
      "\n",
      "Sample combined text:\n",
      "Question: I am still waiting on my card? Answer: Your card typically arrives within 5-7 business days after ordering. \n",
      "    If it's been longer than this, please check your delivery address in the app ...\n",
      "\n",
      "Creating embeddings with text-embedding-3-small...\n",
      "Total texts: 10003\n",
      "Batch size: 100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [02:33<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Created 10003 embeddings\n",
      "   Embedding dimension: 1536\n",
      "   Model: text-embedding-3-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ## 6. Create Embeddings\n",
    "\n",
    "# %%\n",
    "def get_openai_embedding(\n",
    "    text: str,\n",
    "    model: str = \"text-embedding-3-small\"\n",
    ") -> list:\n",
    "    \"\"\"Get embedding from OpenAI\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = client.embeddings.create(\n",
    "        input=[text],\n",
    "        model=model\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def create_embeddings_batch(\n",
    "    texts: list,\n",
    "    model: str = \"text-embedding-3-small\",\n",
    "    batch_size: int = 100\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Create embeddings in batches for efficiency\n",
    "    \n",
    "    Args:\n",
    "        texts: List of texts to embed\n",
    "        model: OpenAI embedding model\n",
    "        batch_size: Number of texts per API call\n",
    "    \n",
    "    Returns:\n",
    "        List of embeddings\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    print(f\"\\nCreating embeddings with {model}...\")\n",
    "    print(f\"Total texts: {len(texts)}\")\n",
    "    print(f\"Batch size: {batch_size}\\n\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding batches\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        # Clean texts\n",
    "        batch = [text.replace(\"\\n\", \" \") for text in batch]\n",
    "        \n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                input=batch,\n",
    "                model=model\n",
    "            )\n",
    "            \n",
    "            batch_embeddings = [data.embedding for data in response.data]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error in batch {i//batch_size}: {e}\")\n",
    "            # Add zero vectors as fallback\n",
    "            embedding_dim = 1536 if 'small' in model else 3072\n",
    "            all_embeddings.extend([[0.0] * embedding_dim] * len(batch))\n",
    "        \n",
    "        # Rate limiting\n",
    "        if (i + batch_size) % 1000 == 0:\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "# %%\n",
    "# Combine question and answer for richer embeddings\n",
    "print(\"Preparing texts for embedding...\")\n",
    "kb_df['combined_text'] = (\n",
    "    \"Question: \" + kb_df['text'] +\n",
    "    \" Answer: \" + kb_df['answer']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Combined {len(kb_df)} question-answer pairs\")\n",
    "print(f\"\\nSample combined text:\")\n",
    "print(kb_df['combined_text'].iloc[0][:200] + \"...\")\n",
    "\n",
    "# %%\n",
    "# Create embeddings\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "embeddings = create_embeddings_batch(\n",
    "    kb_df['combined_text'].tolist(),\n",
    "    model=EMBEDDING_MODEL,\n",
    "    batch_size=100\n",
    ")\n",
    "\n",
    "# Add to dataframe\n",
    "kb_df['embedding'] = embeddings\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(embeddings)} embeddings\")\n",
    "print(f\"   Embedding dimension: {len(embeddings[0])}\")\n",
    "print(f\"   Model: {EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72ed4e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database directory: c:\\Users\\victo\\customer-support-rag\\data\\vector_db\n",
      "Deleted existing collection\n",
      "‚úÖ ChromaDB collection created: 'banking_support'\n",
      "\n",
      "Preparing data for vector database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10003/10003 [00:00<00:00, 22125.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prepared 10003 documents\n",
      "\n",
      "Adding to vector database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding to DB: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101/101 [00:15<00:00,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Vector database created successfully!\n",
      "   Total entries: 10003\n",
      "   Location: c:\\Users\\victo\\customer-support-rag\\data\\vector_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ## 7. Build Vector Database (ChromaDB)\n",
    "\n",
    "# %%\n",
    "# Create vector database directory\n",
    "vector_db_dir = project_root / 'data' / 'vector_db'\n",
    "vector_db_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Vector database directory: {vector_db_dir}\")\n",
    "\n",
    "# Initialize ChromaDB\n",
    "chroma_client = chromadb.PersistentClient(path=str(vector_db_dir))\n",
    "\n",
    "# Delete existing collection if it exists\n",
    "try:\n",
    "    chroma_client.delete_collection(name=\"banking_support\")\n",
    "    print(\"Deleted existing collection\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new collection with enhanced metadata\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"banking_support\",\n",
    "    metadata={\n",
    "        \"description\": \"Banking customer support Q&A\",\n",
    "        \"embedding_model\": EMBEDDING_MODEL,\n",
    "        \"answer_model\": \"gpt-4o-mini\",\n",
    "        \"total_entries\": len(kb_df),\n",
    "        \"version\": \"2.0\",\n",
    "        \"created_date\": datetime.now().isoformat(),\n",
    "        \"stratified_sampling\": USE_STRATIFIED_SAMPLING if USE_SAMPLE else False,\n",
    "        \"template_count\": kb_df['category'].isin(ANSWER_TEMPLATES.keys()).sum()\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ ChromaDB collection created: 'banking_support'\")\n",
    "\n",
    "# %%\n",
    "# Prepare data for ChromaDB\n",
    "print(\"\\nPreparing data for vector database...\")\n",
    "\n",
    "documents = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "embeddings_list = []\n",
    "\n",
    "for idx, row in tqdm(kb_df.iterrows(), total=len(kb_df), desc=\"Preparing data\"):\n",
    "    # Document text (what will be returned in search)\n",
    "    doc_text = f\"Question: {row['text']}\\nAnswer: {row['answer']}\"\n",
    "    documents.append(doc_text)\n",
    "    \n",
    "    # Enhanced metadata\n",
    "    metadata = {\n",
    "        'question': row['text'],\n",
    "        'answer': row['answer'],\n",
    "        'category': row['category'],\n",
    "        'category_id': int(row['label']),\n",
    "        'word_count': int(row['word_count']),\n",
    "        'answer_length': int(row['answer_length']),\n",
    "        'has_negation': bool(row['has_negation']),\n",
    "        'is_complex': bool(row['is_complex']),\n",
    "        'question_type': row['question_type']\n",
    "    }\n",
    "    metadatas.append(metadata)\n",
    "    \n",
    "    # Unique ID\n",
    "    ids.append(f\"kb_{idx}\")\n",
    "    \n",
    "    # Embedding\n",
    "    embeddings_list.append(row['embedding'])\n",
    "\n",
    "print(f\"‚úÖ Prepared {len(documents)} documents\")\n",
    "\n",
    "# %%\n",
    "# Add to ChromaDB in batches\n",
    "print(\"\\nAdding to vector database...\")\n",
    "\n",
    "batch_size = 100\n",
    "for i in tqdm(range(0, len(documents), batch_size), desc=\"Adding to DB\"):\n",
    "    batch_docs = documents[i:i+batch_size]\n",
    "    batch_meta = metadatas[i:i+batch_size]\n",
    "    batch_ids = ids[i:i+batch_size]\n",
    "    batch_emb = embeddings_list[i:i+batch_size]\n",
    "    \n",
    "    collection.add(\n",
    "        documents=batch_docs,\n",
    "        metadatas=batch_meta,\n",
    "        ids=batch_ids,\n",
    "        embeddings=batch_emb\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÖ Vector database created successfully!\")\n",
    "print(f\"   Total entries: {collection.count()}\")\n",
    "print(f\"   Location: {vector_db_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cebd87f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced retrieval functions defined\n",
      "   - Query preprocessing with negation enhancement\n",
      "   - Smart hybrid search (adapts to dataset size)\n",
      "   - Category-aware boosting\n"
     ]
    }
   ],
   "source": [
    "# ## 8. Enhanced Retrieval Functions with Query Preprocessing\n",
    "\n",
    "# %%\n",
    "def preprocess_query(query: str, enhance_negation: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess query for better retrieval\n",
    "    \n",
    "    Args:\n",
    "        query: Original query text\n",
    "        enhance_negation: Whether to enhance negation queries\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed query string\n",
    "    \"\"\"\n",
    "    # Detect negation\n",
    "    negation_words = ['not', 'no', \"n't\", 'never', 'none', 'nobody', 'nothing', \n",
    "                      'nowhere', 'neither', 'hasn\\'t', 'haven\\'t', 'didn\\'t', 'don\\'t',\n",
    "                      'won\\'t', 'wouldn\\'t', 'couldn\\'t', 'shouldn\\'t', 'isn\\'t', 'aren\\'t']\n",
    "    \n",
    "    has_negation = any(neg in query.lower() for neg in negation_words)\n",
    "    \n",
    "    # Enhance negation queries for better semantic matching\n",
    "    if has_negation and enhance_negation:\n",
    "        # Add context that this is a problem/issue\n",
    "        query = query + \" [PROBLEM/ISSUE/NOT_WORKING]\"\n",
    "    \n",
    "    # Clean up extra whitespace\n",
    "    query = ' '.join(query.split())\n",
    "    \n",
    "    return query\n",
    "\n",
    "# %%\n",
    "def search_knowledge_base(query: str, n_results: int = 5):\n",
    "    \"\"\"Basic semantic search with query preprocessing\"\"\"\n",
    "    \n",
    "    # Preprocess query\n",
    "    processed_query = preprocess_query(query)\n",
    "    \n",
    "    # Embed the processed query\n",
    "    query_embedding = get_openai_embedding(processed_query, model=EMBEDDING_MODEL)\n",
    "    \n",
    "    # Search\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def hybrid_search_smart(query: str, n_results: int = 5, alpha: float = 0.7):\n",
    "    \"\"\"\n",
    "    Smart hybrid search that adapts to dataset size\n",
    "    Falls back to semantic-only for small datasets\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        n_results: Number of results to return\n",
    "        alpha: Weight for dense retrieval (1-alpha for sparse)\n",
    "    \n",
    "    Returns:\n",
    "        Combined results\n",
    "    \"\"\"\n",
    "    # Check if we have enough data for hybrid search\n",
    "    if not BM25_AVAILABLE:\n",
    "        print(\"‚ö†Ô∏è BM25 not available, using semantic search\")\n",
    "        return search_knowledge_base(query, n_results=n_results)\n",
    "    \n",
    "    if len(kb_df) < 1000:\n",
    "        # Dataset too small for effective hybrid search\n",
    "        return search_knowledge_base(query, n_results=n_results)\n",
    "    \n",
    "    # Preprocess query\n",
    "    processed_query = preprocess_query(query)\n",
    "    \n",
    "    # Dense retrieval (semantic)\n",
    "    dense_results = search_knowledge_base(processed_query, n_results=20)\n",
    "    \n",
    "    # Prepare corpus for BM25\n",
    "    corpus = [doc.split() for doc in documents]\n",
    "    bm25 = BM25Okapi(corpus)\n",
    "    \n",
    "    # Sparse retrieval (keyword-based)\n",
    "    tokenized_query = processed_query.split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Normalize scores to 0-1 range\n",
    "    dense_scores = np.array(dense_results['distances'][0])\n",
    "    dense_scores = 1 - dense_scores  # Convert distance to similarity\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    bm25_range = bm25_scores.max() - bm25_scores.min()\n",
    "    if bm25_range > 0:\n",
    "        bm25_scores_norm = (bm25_scores - bm25_scores.min()) / bm25_range\n",
    "    else:\n",
    "        bm25_scores_norm = np.zeros_like(bm25_scores)\n",
    "    \n",
    "    # Combine scores\n",
    "    final_scores = {}\n",
    "    for i, doc_id in enumerate(dense_results['ids'][0]):\n",
    "        idx = int(doc_id.split('_')[1])\n",
    "        dense_score = dense_scores[i] if i < len(dense_scores) else 0\n",
    "        sparse_score = bm25_scores_norm[idx] if idx < len(bm25_scores_norm) else 0\n",
    "        final_scores[doc_id] = alpha * dense_score + (1 - alpha) * sparse_score\n",
    "    \n",
    "    # Sort by combined score\n",
    "    sorted_ids = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:n_results]\n",
    "    \n",
    "    # Retrieve full results for top documents\n",
    "    top_ids = [doc_id for doc_id, score in sorted_ids]\n",
    "    results = collection.get(ids=top_ids, include=['documents', 'metadatas', 'embeddings'])\n",
    "    \n",
    "    return {\n",
    "        'ids': [top_ids],\n",
    "        'documents': [results['documents']],\n",
    "        'metadatas': [results['metadatas']],\n",
    "        'distances': [[1 - final_scores[doc_id] for doc_id in top_ids]]  # Convert back to distance\n",
    "    }\n",
    "\n",
    "def category_aware_search(query: str, detected_category: str = None, n_results: int = 5, boost: float = 1.3):\n",
    "    \"\"\"\n",
    "    Search with optional category boosting\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        detected_category: If provided, boost results from this category\n",
    "        n_results: Number of results to return\n",
    "        boost: Multiplier for category matches\n",
    "    \n",
    "    Returns:\n",
    "        Search results\n",
    "    \"\"\"\n",
    "    # Get more results initially\n",
    "    base_results = search_knowledge_base(query, n_results=20)\n",
    "    \n",
    "    if detected_category:\n",
    "        # Calculate boosted scores\n",
    "        boosted_results = []\n",
    "        for i, (doc_id, doc, metadata, distance) in enumerate(zip(\n",
    "            base_results['ids'][0],\n",
    "            base_results['documents'][0],\n",
    "            base_results['metadatas'][0],\n",
    "            base_results['distances'][0]\n",
    "        )):\n",
    "            # Convert distance to similarity\n",
    "            similarity = 1 - distance\n",
    "            \n",
    "            # Boost if category matches\n",
    "            if metadata['category'] == detected_category:\n",
    "                similarity *= boost\n",
    "            \n",
    "            boosted_results.append((doc_id, doc, metadata, 1 - similarity))  # Convert back to distance\n",
    "        \n",
    "        # Sort by boosted scores\n",
    "        boosted_results.sort(key=lambda x: x[3])\n",
    "        \n",
    "        # Return top n\n",
    "        return {\n",
    "            'ids': [[r[0] for r in boosted_results[:n_results]]],\n",
    "            'documents': [[r[1] for r in boosted_results[:n_results]]],\n",
    "            'metadatas': [[r[2] for r in boosted_results[:n_results]]],\n",
    "            'distances': [[r[3] for r in boosted_results[:n_results]]]\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'ids': [base_results['ids'][0][:n_results]],\n",
    "        'documents': [base_results['documents'][0][:n_results]],\n",
    "        'metadatas': [base_results['metadatas'][0][:n_results]],\n",
    "        'distances': [base_results['distances'][0][:n_results]]\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Enhanced retrieval functions defined\")\n",
    "print(f\"   - Query preprocessing with negation enhancement\")\n",
    "print(f\"   - Smart hybrid search (adapts to dataset size)\")\n",
    "print(f\"   - Category-aware boosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "236652ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä LOADING TEST DATA FOR REAL EVALUATION\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Test set loaded: 3,080 queries\n",
      "   ‚ö†Ô∏è  These queries are UNSEEN by the knowledge base!\n",
      "   ‚úÖ This is the PROPER way to evaluate performance\n",
      "\n",
      "‚ö†Ô∏è  Test data missing preprocessing columns - adding them now...\n",
      "   ‚úÖ Added preprocessing columns to test data\n",
      "\n",
      "Test data characteristics:\n",
      "   Categories: 77\n",
      "   Avg query length: 11.0 words\n",
      "   Negation queries: 802 (26.0%)\n",
      "   Complex queries: 500 (16.2%)\n",
      "\n",
      "Sample test queries:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>word_count</th>\n",
       "      <th>has_negation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do I locate my card?</td>\n",
       "      <td>card_arrival</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I still have not received my new card, I order...</td>\n",
       "      <td>card_arrival</td>\n",
       "      <td>14</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I ordered a card but it has not arrived. Help ...</td>\n",
       "      <td>card_arrival</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is there a way to know when my card will arrive?</td>\n",
       "      <td>card_arrival</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My card has not arrived yet.</td>\n",
       "      <td>card_arrival</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      category  \\\n",
       "0                           How do I locate my card?  card_arrival   \n",
       "1  I still have not received my new card, I order...  card_arrival   \n",
       "2  I ordered a card but it has not arrived. Help ...  card_arrival   \n",
       "3   Is there a way to know when my card will arrive?  card_arrival   \n",
       "4                       My card has not arrived yet.  card_arrival   \n",
       "\n",
       "   word_count  has_negation  \n",
       "0           6         False  \n",
       "1          14          True  \n",
       "2          11          True  \n",
       "3          11          True  \n",
       "4           6          True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ## 9. Load and Preprocess Test Data for REAL Evaluation\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä LOADING TEST DATA FOR REAL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load test data (NEVER seen by the knowledge base)\n",
    "test_df = pd.read_csv('../data/processed/test_processed.csv')\n",
    "\n",
    "print(f\"\\n‚úÖ Test set loaded: {len(test_df):,} queries\")\n",
    "print(f\"   ‚ö†Ô∏è  These queries are UNSEEN by the knowledge base!\")\n",
    "print(f\"   ‚úÖ This is the PROPER way to evaluate performance\")\n",
    "\n",
    "# Check if preprocessing columns exist\n",
    "if 'has_negation' not in test_df.columns:\n",
    "    print(\"\\n‚ö†Ô∏è  Test data missing preprocessing columns - adding them now...\")\n",
    "    \n",
    "    # Add the same preprocessing columns as training data\n",
    "    negation_words = ['not', 'no', \"n't\", 'never', 'none', 'nobody', 'nothing', \n",
    "                      'nowhere', 'neither', 'hasn\\'t', 'haven\\'t', 'didn\\'t', 'don\\'t']\n",
    "    test_df['has_negation'] = test_df['text'].str.lower().str.contains('|'.join(negation_words), regex=True)\n",
    "    \n",
    "    # Complex query detection\n",
    "    test_df['has_and'] = test_df['text'].str.lower().str.contains(r'\\band\\b')\n",
    "    test_df['has_or'] = test_df['text'].str.lower().str.contains(r'\\bor\\b')\n",
    "    test_df['has_but'] = test_df['text'].str.lower().str.contains(r'\\bbut\\b')\n",
    "    test_df['has_multiple_sentences'] = test_df['text'].str.contains(r'[.!?]\\s+[A-Z]')\n",
    "    \n",
    "    # Define complex as: >15 words OR (has_and + has_or) OR (has_negation + connector words)\n",
    "    test_df['is_complex'] = (\n",
    "        (test_df['word_count'] > 15) |\n",
    "        (test_df['has_and'] & test_df['has_or']) |\n",
    "        (test_df['has_negation'] & (test_df['has_and'] | test_df['has_or'] | test_df['has_but']))\n",
    "    )\n",
    "    \n",
    "    print(\"   ‚úÖ Added preprocessing columns to test data\")\n",
    "\n",
    "print(f\"\\nTest data characteristics:\")\n",
    "print(f\"   Categories: {test_df['category'].nunique()}\")\n",
    "print(f\"   Avg query length: {test_df['word_count'].mean():.1f} words\")\n",
    "print(f\"   Negation queries: {test_df['has_negation'].sum()} ({test_df['has_negation'].sum()/len(test_df)*100:.1f}%)\")\n",
    "print(f\"   Complex queries: {test_df['is_complex'].sum()} ({test_df['is_complex'].sum()/len(test_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nSample test queries:\")\n",
    "display(test_df[['text', 'category', 'word_count', 'has_negation']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97035387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation framework defined\n",
      "\n",
      "‚úÖ Created REAL evaluation set with 100 UNSEEN test cases:\n",
      "   Negation: 20\n",
      "   Complex: 20\n",
      "   Short: 20\n",
      "   Long: 20\n",
      "   Baseline: 20\n",
      "\n",
      "‚úÖ Saved real evaluation set to: c:\\Users\\victo\\customer-support-rag\\data\\processed\\real_evaluation_set.json\n"
     ]
    }
   ],
   "source": [
    "# ## 10. Create Evaluation Framework\n",
    "\n",
    "# %%\n",
    "def create_real_evaluation_set(test_df, samples_per_type=20):\n",
    "    \"\"\"\n",
    "    Create evaluation set from UNSEEN test data\n",
    "    \n",
    "    Args:\n",
    "        test_df: Test dataframe (unseen queries)\n",
    "        samples_per_type: Samples per test category\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation test set with unseen queries\n",
    "    \"\"\"\n",
    "    test_set = []\n",
    "    \n",
    "    # 1. Negation queries from test set\n",
    "    negation_df = test_df[test_df['has_negation'] == True].sample(\n",
    "        min(samples_per_type, test_df['has_negation'].sum()),\n",
    "        random_state=42\n",
    "    )\n",
    "    for idx, row in negation_df.iterrows():\n",
    "        test_set.append({\n",
    "            'query': row['text'],\n",
    "            'expected_category': row['category'],\n",
    "            'has_negation': True,\n",
    "            'test_type': 'negation'\n",
    "        })\n",
    "    \n",
    "    # 2. Complex queries from test set\n",
    "    complex_df = test_df[test_df['is_complex'] == True].sample(\n",
    "        min(samples_per_type, test_df['is_complex'].sum()),\n",
    "        random_state=42\n",
    "    )\n",
    "    for idx, row in complex_df.iterrows():\n",
    "        test_set.append({\n",
    "            'query': row['text'],\n",
    "            'expected_category': row['category'],\n",
    "            'has_negation': row['has_negation'],\n",
    "            'test_type': 'complex'\n",
    "        })\n",
    "    \n",
    "    # 3. Short queries from test set\n",
    "    short_df = test_df[test_df['word_count'] <= 5].sample(\n",
    "        min(samples_per_type, (test_df['word_count'] <= 5).sum()),\n",
    "        random_state=42\n",
    "    )\n",
    "    for idx, row in short_df.iterrows():\n",
    "        test_set.append({\n",
    "            'query': row['text'],\n",
    "            'expected_category': row['category'],\n",
    "            'has_negation': row['has_negation'],\n",
    "            'test_type': 'short'\n",
    "        })\n",
    "    \n",
    "    # 4. Long queries from test set\n",
    "    long_df = test_df[test_df['word_count'] > 20].sample(\n",
    "        min(samples_per_type, (test_df['word_count'] > 20).sum()),\n",
    "        random_state=42\n",
    "    )\n",
    "    for idx, row in long_df.iterrows():\n",
    "        test_set.append({\n",
    "            'query': row['text'],\n",
    "            'expected_category': row['category'],\n",
    "            'has_negation': row['has_negation'],\n",
    "            'test_type': 'long'\n",
    "        })\n",
    "    \n",
    "    # 5. Random baseline from test set\n",
    "    random_df = test_df.sample(min(samples_per_type, len(test_df)), random_state=42)\n",
    "    for idx, row in random_df.iterrows():\n",
    "        test_set.append({\n",
    "            'query': row['text'],\n",
    "            'expected_category': row['category'],\n",
    "            'has_negation': row['has_negation'],\n",
    "            'test_type': 'baseline'\n",
    "        })\n",
    "    \n",
    "    print(f\"\\n‚úÖ Created REAL evaluation set with {len(test_set)} UNSEEN test cases:\")\n",
    "    print(f\"   Negation: {sum(1 for t in test_set if t['test_type'] == 'negation')}\")\n",
    "    print(f\"   Complex: {sum(1 for t in test_set if t['test_type'] == 'complex')}\")\n",
    "    print(f\"   Short: {sum(1 for t in test_set if t['test_type'] == 'short')}\")\n",
    "    print(f\"   Long: {sum(1 for t in test_set if t['test_type'] == 'long')}\")\n",
    "    print(f\"   Baseline: {sum(1 for t in test_set if t['test_type'] == 'baseline')}\")\n",
    "    \n",
    "    return test_set\n",
    "\n",
    "def evaluate_retrieval(test_set, search_function, k_values=[1, 3, 5], verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluate retrieval performance\n",
    "    \n",
    "    Args:\n",
    "        test_set: List of test cases\n",
    "        search_function: Function to use for retrieval\n",
    "        k_values: k values to evaluate accuracy@k\n",
    "        verbose: Print detailed results\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    results = {f'accuracy@{k}': 0 for k in k_values}\n",
    "    category_correct = 0\n",
    "    results_by_type = defaultdict(lambda: {f'accuracy@{k}': 0 for k in k_values})\n",
    "    results_by_type_category = defaultdict(int)\n",
    "    \n",
    "    print(f\"\\nEvaluating {len(test_set)} test cases...\")\n",
    "    \n",
    "    for test_case in tqdm(test_set, desc=\"Evaluating\"):\n",
    "        query = test_case['query']\n",
    "        expected_category = test_case['expected_category']\n",
    "        test_type = test_case['test_type']\n",
    "        \n",
    "        # Retrieve results\n",
    "        retrieved = search_function(query, n_results=max(k_values))\n",
    "        \n",
    "        # Check accuracy@k\n",
    "        for k in k_values:\n",
    "            top_k_categories = [meta['category'] for meta in retrieved['metadatas'][0][:k]]\n",
    "            if expected_category in top_k_categories:\n",
    "                results[f'accuracy@{k}'] += 1\n",
    "                results_by_type[test_type][f'accuracy@{k}'] += 1\n",
    "        \n",
    "        # Check category accuracy (top-1)\n",
    "        top_category = retrieved['metadatas'][0][0]['category']\n",
    "        if top_category == expected_category:\n",
    "            category_correct += 1\n",
    "            results_by_type_category[test_type] += 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nQuery: {query[:60]}...\")\n",
    "            print(f\"Expected: {expected_category}\")\n",
    "            print(f\"Got: {top_category}\")\n",
    "            print(f\"Match: {'‚úÖ' if top_category == expected_category else '‚ùå'}\")\n",
    "    \n",
    "    # Calculate percentages\n",
    "    n = len(test_set)\n",
    "    for k in k_values:\n",
    "        results[f'accuracy@{k}'] = results[f'accuracy@{k}'] / n\n",
    "    results['category_accuracy'] = category_correct / n\n",
    "    \n",
    "    # Calculate by type\n",
    "    type_counts = defaultdict(int)\n",
    "    for test_case in test_set:\n",
    "        type_counts[test_case['test_type']] += 1\n",
    "    \n",
    "    for test_type, count in type_counts.items():\n",
    "        for k in k_values:\n",
    "            results_by_type[test_type][f'accuracy@{k}'] /= count\n",
    "        results_by_type[test_type]['category_accuracy'] = results_by_type_category[test_type] / count\n",
    "    \n",
    "    results['by_type'] = dict(results_by_type)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_evaluation_results(results, title=\"EVALUATION RESULTS\"):\n",
    "    \"\"\"Pretty print evaluation results\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"üìä {title}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n Overall Performance:\")\n",
    "    print(f\"   Accuracy@1: {results['accuracy@1']:.1%}\")\n",
    "    print(f\"   Accuracy@3: {results['accuracy@3']:.1%}\")\n",
    "    print(f\"   Accuracy@5: {results['accuracy@5']:.1%}\")\n",
    "    print(f\"   Category Accuracy: {results['category_accuracy']:.1%}\")\n",
    "    \n",
    "    print(\"\\n Performance by Query Type:\")\n",
    "    for test_type, metrics in results['by_type'].items():\n",
    "        print(f\"\\n   {test_type.upper()}:\")\n",
    "        print(f\"      Accuracy@1: {metrics['accuracy@1']:.1%}\")\n",
    "        print(f\"      Accuracy@3: {metrics['accuracy@3']:.1%}\")\n",
    "        print(f\"      Category Accuracy: {metrics['category_accuracy']:.1%}\")\n",
    "        \n",
    "        # Flag issues\n",
    "        if test_type == 'negation' and metrics['category_accuracy'] < 0.8:\n",
    "            print(f\"      ‚ö†Ô∏è  WARNING: Low negation accuracy!\")\n",
    "        if test_type == 'complex' and metrics['category_accuracy'] < 0.75:\n",
    "            print(f\"      ‚ö†Ô∏è  WARNING: Low complex query accuracy!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"‚úÖ Evaluation framework defined\")\n",
    "\n",
    "# %%\n",
    "# Create REAL evaluation set from UNSEEN test data\n",
    "real_eval_set = create_real_evaluation_set(test_df, samples_per_type=20)\n",
    "\n",
    "# Save it\n",
    "real_eval_path = project_root / 'data' / 'processed' / 'real_evaluation_set.json'\n",
    "with open(real_eval_path, 'w') as f:\n",
    "    json.dump(real_eval_set, f, indent=2)\n",
    "print(f\"\\n‚úÖ Saved real evaluation set to: {real_eval_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "224eba42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üß™ REAL EVALUATION - BASIC SEMANTIC SEARCH\n",
      "Testing on UNSEEN queries from test set\n",
      "======================================================================\n",
      "\n",
      "Evaluating 100 test cases...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:39<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä REAL EVALUATION - UNSEEN DATA\n",
      "======================================================================\n",
      "\n",
      " Overall Performance:\n",
      "   Accuracy@1: 89.0%\n",
      "   Accuracy@3: 94.0%\n",
      "   Accuracy@5: 96.0%\n",
      "   Category Accuracy: 89.0%\n",
      "\n",
      " Performance by Query Type:\n",
      "\n",
      "   NEGATION:\n",
      "      Accuracy@1: 90.0%\n",
      "      Accuracy@3: 90.0%\n",
      "      Category Accuracy: 90.0%\n",
      "\n",
      "   COMPLEX:\n",
      "      Accuracy@1: 85.0%\n",
      "      Accuracy@3: 95.0%\n",
      "      Category Accuracy: 85.0%\n",
      "\n",
      "   SHORT:\n",
      "      Accuracy@1: 85.0%\n",
      "      Accuracy@3: 90.0%\n",
      "      Category Accuracy: 85.0%\n",
      "\n",
      "   LONG:\n",
      "      Accuracy@1: 90.0%\n",
      "      Accuracy@3: 100.0%\n",
      "      Category Accuracy: 90.0%\n",
      "\n",
      "   BASELINE:\n",
      "      Accuracy@1: 95.0%\n",
      "      Accuracy@3: 95.0%\n",
      "      Category Accuracy: 95.0%\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üß™ REAL EVALUATION - HYBRID SEARCH\n",
      "Testing on UNSEEN queries from test set\n",
      "======================================================================\n",
      "\n",
      "Evaluating 100 test cases...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:18<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä REAL EVALUATION - HYBRID (UNSEEN DATA)\n",
      "======================================================================\n",
      "\n",
      " Overall Performance:\n",
      "   Accuracy@1: 83.0%\n",
      "   Accuracy@3: 95.0%\n",
      "   Accuracy@5: 96.0%\n",
      "   Category Accuracy: 83.0%\n",
      "\n",
      " Performance by Query Type:\n",
      "\n",
      "   NEGATION:\n",
      "      Accuracy@1: 80.0%\n",
      "      Accuracy@3: 90.0%\n",
      "      Category Accuracy: 80.0%\n",
      "\n",
      "   COMPLEX:\n",
      "      Accuracy@1: 90.0%\n",
      "      Accuracy@3: 100.0%\n",
      "      Category Accuracy: 90.0%\n",
      "\n",
      "   SHORT:\n",
      "      Accuracy@1: 70.0%\n",
      "      Accuracy@3: 90.0%\n",
      "      Category Accuracy: 70.0%\n",
      "\n",
      "   LONG:\n",
      "      Accuracy@1: 90.0%\n",
      "      Accuracy@3: 100.0%\n",
      "      Category Accuracy: 90.0%\n",
      "\n",
      "   BASELINE:\n",
      "      Accuracy@1: 85.0%\n",
      "      Accuracy@3: 95.0%\n",
      "      Category Accuracy: 85.0%\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìä SEMANTIC vs HYBRID COMPARISON\n",
      "======================================================================\n",
      "Semantic Accuracy@1: 89.0%\n",
      "Hybrid Accuracy@1:   83.0%\n",
      "‚ö†Ô∏è Hybrid decreases by: -6.0%\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ## 11. Run REAL Evaluation on Unseen Data\n",
    "\n",
    "# %%\n",
    "# Evaluate basic semantic search on REAL unseen data\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üß™ REAL EVALUATION - BASIC SEMANTIC SEARCH\")\n",
    "print(\"Testing on UNSEEN queries from test set\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "real_results = evaluate_retrieval(\n",
    "    real_eval_set, \n",
    "    search_knowledge_base, \n",
    "    k_values=[1, 3, 5],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print_evaluation_results(real_results, \"REAL EVALUATION - UNSEEN DATA\")\n",
    "\n",
    "# %%\n",
    "# Only evaluate hybrid search if we have enough data\n",
    "if USE_HYBRID_SEARCH and BM25_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üß™ REAL EVALUATION - HYBRID SEARCH\")\n",
    "    print(\"Testing on UNSEEN queries from test set\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    hybrid_real_results = evaluate_retrieval(\n",
    "        real_eval_set,\n",
    "        hybrid_search_smart,\n",
    "        k_values=[1, 3, 5],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print_evaluation_results(hybrid_real_results, \"REAL EVALUATION - HYBRID (UNSEEN DATA)\")\n",
    "    \n",
    "    # Compare performance\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä SEMANTIC vs HYBRID COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Semantic Accuracy@1: {real_results['accuracy@1']:.1%}\")\n",
    "    print(f\"Hybrid Accuracy@1:   {hybrid_real_results['accuracy@1']:.1%}\")\n",
    "    improvement = hybrid_real_results['accuracy@1'] - real_results['accuracy@1']\n",
    "    if improvement > 0:\n",
    "        print(f\"‚úÖ Hybrid improves by: +{improvement:.1%}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Hybrid decreases by: {improvement:.1%}\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Hybrid search evaluation skipped\")\n",
    "    if not BM25_AVAILABLE:\n",
    "        print(\"   Reason: BM25 not available\")\n",
    "    else:\n",
    "        print(\"   Reason: Dataset too small (need 1000+ entries)\")\n",
    "    print(\"   Using semantic results for configuration\")\n",
    "    hybrid_real_results = real_results.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67f853c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üìä ADVANCED METRICS\n",
      "======================================================================\n",
      "\n",
      "Calculating advanced metrics for 100 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing metrics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:36<00:00,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Ranking Quality:\n",
      "   MRR (Mean Reciprocal Rank): 0.915\n",
      "   NDCG: 3.807\n",
      "\n",
      "üí∞ Cost Analysis:\n",
      "   Embedding calls: 100\n",
      "   Embedding cost: $0.0020\n",
      "   Answer generation cost: $0.0100\n",
      "   Total per 100 queries: $0.0120\n",
      "   Projected cost per 1000 queries: $0.12\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ## 12. Advanced Metrics (MRR, NDCG, Cost Analysis)\n",
    "\n",
    "# %%\n",
    "def calculate_additional_metrics(test_set, search_function):\n",
    "    \"\"\"\n",
    "    Calculate additional retrieval metrics\n",
    "    \n",
    "    Args:\n",
    "        test_set: List of test cases\n",
    "        search_function: Function to use for retrieval\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with MRR, NDCG, and cost estimates\n",
    "    \"\"\"\n",
    "    mrr_scores = []\n",
    "    ndcg_scores = []\n",
    "    total_api_calls = 0\n",
    "    \n",
    "    print(f\"\\nCalculating advanced metrics for {len(test_set)} queries...\")\n",
    "    \n",
    "    for test_case in tqdm(test_set, desc=\"Computing metrics\"):\n",
    "        query = test_case['query']\n",
    "        expected_category = test_case['expected_category']\n",
    "        \n",
    "        # Retrieve results\n",
    "        results = search_function(query, n_results=10)\n",
    "        total_api_calls += 1\n",
    "        \n",
    "        # Calculate MRR (Mean Reciprocal Rank)\n",
    "        retrieved_categories = [meta['category'] for meta in results['metadatas'][0]]\n",
    "        try:\n",
    "            rank = retrieved_categories.index(expected_category) + 1\n",
    "            mrr_scores.append(1.0 / rank)\n",
    "        except ValueError:\n",
    "            mrr_scores.append(0.0)\n",
    "        \n",
    "        # Calculate NDCG (simplified binary relevance)\n",
    "        relevance = [1 if cat == expected_category else 0 for cat in retrieved_categories]\n",
    "        dcg = sum([rel / np.log2(i + 2) for i, rel in enumerate(relevance)])\n",
    "        idcg = 1.0  # Best case: relevant item at position 1\n",
    "        ndcg_scores.append(dcg / idcg if idcg > 0 else 0.0)\n",
    "    \n",
    "    # Calculate costs\n",
    "    cost_per_embedding = 0.00002  # $0.02 per 1M tokens, ~1 token per query\n",
    "    cost_per_answer = 0.0001  # GPT-4o-mini cost estimate\n",
    "    \n",
    "    estimated_cost = {\n",
    "        'embedding_calls': total_api_calls,\n",
    "        'embedding_cost': total_api_calls * cost_per_embedding,\n",
    "        'answer_cost': total_api_calls * cost_per_answer,\n",
    "        'total_cost': total_api_calls * (cost_per_embedding + cost_per_answer)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'mrr': np.mean(mrr_scores),\n",
    "        'ndcg': np.mean(ndcg_scores),\n",
    "        'cost': estimated_cost\n",
    "    }\n",
    "\n",
    "# Add to evaluation results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä ADVANCED METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "advanced_metrics = calculate_additional_metrics(real_eval_set, search_knowledge_base)\n",
    "\n",
    "print(f\"\\nüìà Ranking Quality:\")\n",
    "print(f\"   MRR (Mean Reciprocal Rank): {advanced_metrics['mrr']:.3f}\")\n",
    "print(f\"   NDCG: {advanced_metrics['ndcg']:.3f}\")\n",
    "\n",
    "print(f\"\\nüí∞ Cost Analysis:\")\n",
    "print(f\"   Embedding calls: {advanced_metrics['cost']['embedding_calls']}\")\n",
    "print(f\"   Embedding cost: ${advanced_metrics['cost']['embedding_cost']:.4f}\")\n",
    "print(f\"   Answer generation cost: ${advanced_metrics['cost']['answer_cost']:.4f}\")\n",
    "print(f\"   Total per {len(real_eval_set)} queries: ${advanced_metrics['cost']['total_cost']:.4f}\")\n",
    "print(f\"   Projected cost per 1000 queries: ${advanced_metrics['cost']['total_cost'] * 1000 / len(real_eval_set):.2f}\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7581cd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üéØ PERFORMANCE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üìä Real Performance Metrics:\n",
      "   Overall Accuracy@1: 89.0%\n",
      "   Overall Accuracy@5: 96.0%\n",
      "   Negation Accuracy: 90.0%\n",
      "   Complex Query Accuracy: 85.0%\n",
      "   MRR: 0.915\n",
      "   NDCG: 3.807\n",
      "\n",
      "üìà Knowledge Base Stats:\n",
      "   KB Size: 10003 entries\n",
      "   Categories: 77/77\n",
      "   Avg entries per category: 129.9\n",
      "\n",
      "üîç Analysis:\n",
      "   ‚úÖ EXCELLENT: >70% accuracy even with limited KB!\n",
      "   Your embeddings are working extremely well.\n",
      "\n",
      "üí° Recommendations:\n",
      "   ‚úÖ Negation handling is good!\n",
      "   üìä Hybrid search underperforming with small dataset\n",
      "   Normal with limited examples per category - will improve with full data\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ## 13. Reality Check & Analysis\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéØ PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä Real Performance Metrics:\")\n",
    "print(f\"   Overall Accuracy@1: {real_results['accuracy@1']:.1%}\")\n",
    "print(f\"   Overall Accuracy@5: {real_results['accuracy@5']:.1%}\")\n",
    "print(f\"   Negation Accuracy: {real_results['by_type']['negation']['category_accuracy']:.1%}\")\n",
    "print(f\"   Complex Query Accuracy: {real_results['by_type']['complex']['category_accuracy']:.1%}\")\n",
    "print(f\"   MRR: {advanced_metrics['mrr']:.3f}\")\n",
    "print(f\"   NDCG: {advanced_metrics['ndcg']:.3f}\")\n",
    "\n",
    "print(f\"\\nüìà Knowledge Base Stats:\")\n",
    "print(f\"   KB Size: {len(kb_df)} entries\")\n",
    "print(f\"   Categories: {kb_df['category'].nunique()}/77\")\n",
    "print(f\"   Avg entries per category: {len(kb_df)/kb_df['category'].nunique():.1f}\")\n",
    "\n",
    "print(\"\\nüîç Analysis:\")\n",
    "if real_results['accuracy@1'] >= 0.7:\n",
    "    print(\"   ‚úÖ EXCELLENT: >70% accuracy even with limited KB!\")\n",
    "    print(\"   Your embeddings are working extremely well.\")\n",
    "elif real_results['accuracy@1'] >= 0.5:\n",
    "    print(\"   ‚úÖ GOOD: 50-70% accuracy is solid for a small KB\")\n",
    "    print(\"   Full dataset will push this to 75-85%\")\n",
    "elif real_results['accuracy@1'] >= 0.3:\n",
    "    print(\"   üìä EXPECTED: 30-50% is normal with limited examples per category\")\n",
    "    print(f\"   With {len(kb_df)} KB entries and {kb_df['category'].nunique()} categories\")\n",
    "    print(f\"   Average {len(kb_df)/kb_df['category'].nunique():.1f} examples per category\")\n",
    "    print(\"   Full dataset (10,003 entries) will dramatically improve performance\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  LOW: <30% suggests potential issues\")\n",
    "    print(\"   Check: embedding quality, query similarity, category distribution\")\n",
    "\n",
    "print(\"\\nüí° Recommendations:\")\n",
    "\n",
    "if real_results['by_type']['negation']['category_accuracy'] < 0.7:\n",
    "    print(\"   ‚ö†Ô∏è  Negation handling needs attention\")\n",
    "    print(\"      Consider: Query preprocessing or fine-tuned embeddings\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Negation handling is good!\")\n",
    "\n",
    "if USE_HYBRID_SEARCH and hybrid_real_results['accuracy@1'] < real_results['accuracy@1']:\n",
    "    print(\"   üìä Hybrid search underperforming with small dataset\")\n",
    "    print(\"   Normal with limited examples per category - will improve with full data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba2cb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing knowledge base retrieval with diverse queries...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üîç Query: Why was I charged a fee?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "   Result 1 (similarity: 0.451)\n",
      "   Category: transfer_fee_charged\n",
      "   Question: Why was I charged a fee when making this transfer when I shouldn't have been?\n",
      "   Answer: I understand your concern about the unexpected transfer fee. Fees can sometimes apply depending on the type of transfer ...\n",
      "\n",
      "   Result 2 (similarity: 0.389)\n",
      "   Category: extra_charge_on_statement\n",
      "   Question: Why was my account assessed a fee?\n",
      "   Answer: I'm sorry to hear that you've noticed an extra charge on your statement. Fees can occur for various reasons, such as ove...\n",
      "\n",
      "   Result 3 (similarity: 0.386)\n",
      "   Category: cash_withdrawal_charge\n",
      "   Question: Why was I charged a fee when I withdrew money?\n",
      "   Answer: I understand your concern about the withdrawal fee. Typically, fees can occur if you used an ATM outside of our network ...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üîç Query: My card isn't working\n",
      "   ‚ö†Ô∏è  Contains negation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "   Result 1 (similarity: 0.363)\n",
      "   Category: card_not_working\n",
      "   Question: Can you tell me whats going on with my card it seems to be not working?\n",
      "   Answer: I‚Äôm sorry to hear that your card isn‚Äôt working. Here are a few steps you can take to troubleshoot the issue: \n",
      "\n",
      "1. Check ...\n",
      "   üìå This result also contains negation\n",
      "\n",
      "   Result 2 (similarity: 0.352)\n",
      "   Category: card_not_working\n",
      "   Question: WHAT IS THE MAIN REASON OF THIS PROBLEM\n",
      "   Answer: I'm sorry to hear that your card isn't working. The main reasons for a card not functioning could include insufficient f...\n",
      "\n",
      "   Result 3 (similarity: 0.350)\n",
      "   Category: card_not_working\n",
      "   Question: My card won't physically work.\n",
      "   Answer: I'm sorry to hear that your card isn't working. First, please check if the card is damaged or expired. If it looks fine,...\n",
      "   üìå This result also contains negation\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üîç Query: Card payment didn't go through\n",
      "   ‚ö†Ô∏è  Contains negation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "   Result 1 (similarity: 0.431)\n",
      "   Category: reverted_card_payment?\n",
      "   Question: The card payment didn't work\n",
      "   Answer: I'm sorry to hear that your card payment didn't go through. First, please check if your card is activated and that there...\n",
      "   üìå This result also contains negation\n",
      "\n",
      "   Result 2 (similarity: 0.325)\n",
      "   Category: declined_card_payment\n",
      "   Question: I'm not sure why the card payment didn't work.\n",
      "   Answer: Card payments can be declined for several reasons: insufficient funds, exceeded spending limits, expired card, incorrect...\n",
      "   üìå This result also contains negation\n",
      "\n",
      "   Result 3 (similarity: 0.308)\n",
      "   Category: declined_card_payment\n",
      "   Question: The card payment I made didn't work.\n",
      "   Answer: Card payments can be declined for several reasons: insufficient funds, exceeded spending limits, expired card, incorrect...\n",
      "   üìå This result also contains negation\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üîç Query: How do I reset my PIN?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "   Result 1 (similarity: 0.525)\n",
      "   Category: change_pin\n",
      "   Question: How can I reset my PIN?\n",
      "   Answer: You can change your PIN anytime through the app. Go to Settings > Security > Change PIN. \n",
      "    You'll need to enter your ...\n",
      "\n",
      "   Result 2 (similarity: 0.465)\n",
      "   Category: change_pin\n",
      "   Question: Can I reset my PIN?\n",
      "   Answer: You can change your PIN anytime through the app. Go to Settings > Security > Change PIN. \n",
      "    You'll need to enter your ...\n",
      "\n",
      "   Result 3 (similarity: 0.413)\n",
      "   Category: pin_blocked\n",
      "   Question: How can I reset my darned PIN number?\n",
      "   Answer: If you've entered your PIN incorrectly multiple times, your card is temporarily blocked for security. \n",
      "    To unblock it...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üîç Query: I didn't authorize this payment\n",
      "   ‚ö†Ô∏è  Contains negation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "   Result 1 (similarity: 0.180)\n",
      "   Category: card_payment_not_recognised\n",
      "   Question: There's a payment in the app that I didn't authorize.\n",
      "   Answer: If you see an unrecognized payment, first check if it's a merchant with a different trading name than the store name. \n",
      " ...\n",
      "   üìå This result also contains negation\n",
      "\n",
      "   Result 2 (similarity: 0.156)\n",
      "   Category: card_payment_not_recognised\n",
      "   Question: I think that there is a payment on my card that I did not authorize.  I don't recognize the name attached to the transaction.\n",
      "   Answer: If you see an unrecognized payment, first check if it's a merchant with a different trading name than the store name. \n",
      " ...\n",
      "   üìå This result also contains negation\n",
      "\n",
      "   Result 3 (similarity: 0.145)\n",
      "   Category: cash_withdrawal_not_recognised\n",
      "   Question: The app shows a withdrawal that I did not make or authorize.\n",
      "   Answer: I'm sorry to hear that you're seeing an unauthorized withdrawal. Please take the following steps: \n",
      "\n",
      "1. Check if the tran...\n",
      "   üìå This result also contains negation\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üîç Query: My balance hasn't updated\n",
      "   ‚ö†Ô∏è  Contains negation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "   Result 1 (similarity: 0.113)\n",
      "   Category: cash_withdrawal_not_recognised\n",
      "   Question: My app is showing a cash balance that I did not receive.\n",
      "   Answer: I understand your concern about the cash balance not matching your expectations. Please check if there are any pending t...\n",
      "   üìå This result also contains negation\n",
      "\n",
      "   Result 2 (similarity: 0.061)\n",
      "   Category: pending_cash_withdrawal\n",
      "   Question: My balance is not what I thought it was following an ATM withdrawal\n",
      "   Answer: I understand your concern regarding your balance after the ATM withdrawal. It's possible that the transaction is still p...\n",
      "   üìå This result also contains negation\n",
      "\n",
      "   Result 3 (similarity: 0.056)\n",
      "   Category: top_up_reverted\n",
      "   Question: I did a topup successfully but its not showing in balance. I think there is some problem in system.\n",
      "   Answer: I understand your concern about the top-up not reflecting in your balance. First, please check your transaction history ...\n",
      "   üìå This result also contains negation\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üîç Query: I entered the wrong PIN too many times and now my card is blocked, how do I fix this?\n",
      "   ‚ö†Ô∏è  Contains negation\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "   Result 1 (similarity: 0.660)\n",
      "   Category: pin_blocked\n",
      "   Question: I entered a wrong PIN too many times. How can I fix this?\n",
      "   Answer: If you've entered your PIN incorrectly multiple times, your card is temporarily blocked for security. \n",
      "    To unblock it...\n",
      "\n",
      "   Result 2 (similarity: 0.652)\n",
      "   Category: pin_blocked\n",
      "   Question: I entered the wrong pin too many times and now I think it's blocked. What do I do?\n",
      "   Answer: If you've entered your PIN incorrectly multiple times, your card is temporarily blocked for security. \n",
      "    To unblock it...\n",
      "\n",
      "   Result 3 (similarity: 0.619)\n",
      "   Category: pin_blocked\n",
      "   Question: I entered my pin wrong  too many times.\n",
      "   Answer: If you've entered your PIN incorrectly multiple times, your card is temporarily blocked for security. \n",
      "    To unblock it...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ## 14. Test Retrieval with Sample Queries\n",
    "\n",
    "# %%\n",
    "# Test with diverse sample queries\n",
    "test_queries = [\n",
    "    # Fee-related\n",
    "    \"Why was I charged a fee?\",\n",
    "    \n",
    "    # Card issues with negation\n",
    "    \"My card isn't working\",\n",
    "    \"Card payment didn't go through\",\n",
    "    \n",
    "    # PIN management\n",
    "    \"How do I reset my PIN?\",\n",
    "    \n",
    "    # Unauthorized transactions (negation)\n",
    "    \"I didn't authorize this payment\",\n",
    "    \n",
    "    # Balance issues (negation)\n",
    "    \"My balance hasn't updated\",\n",
    "    \n",
    "    # Complex query\n",
    "    \"I entered the wrong PIN too many times and now my card is blocked, how do I fix this?\"\n",
    "]\n",
    "\n",
    "print(\"Testing knowledge base retrieval with diverse queries...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nüîç Query: {query}\")\n",
    "    \n",
    "    # Check if query has negation indicators\n",
    "    has_negation = any(neg in query.lower() for neg in [\"not\", \"didn't\", \"hasn't\", \"never\", \"no\", \"n't\"])\n",
    "    if has_negation:\n",
    "        print(\"   ‚ö†Ô∏è  Contains negation\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Try basic search\n",
    "    results = search_knowledge_base(query, n_results=3)\n",
    "    \n",
    "    for i, (doc, metadata, distance) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    ), 1):\n",
    "        similarity = 1 - distance\n",
    "        print(f\"\\n   Result {i} (similarity: {similarity:.3f})\")\n",
    "        print(f\"   Category: {metadata['category']}\")\n",
    "        print(f\"   Question: {metadata['question']}\")\n",
    "        print(f\"   Answer: {metadata['answer'][:120]}...\")\n",
    "        \n",
    "        if metadata['has_negation']:\n",
    "            print(f\"   üìå This result also contains negation\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c69ade7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving knowledge base to: c:\\Users\\victo\\customer-support-rag\\data\\processed\n",
      "‚úÖ Saved CSV: c:\\Users\\victo\\customer-support-rag\\data\\processed\\knowledge_base_v2.csv\n",
      "‚úÖ Saved pickle: c:\\Users\\victo\\customer-support-rag\\data\\processed\\knowledge_base_v2_with_embeddings.pkl\n",
      "‚úÖ Saved JSON: c:\\Users\\victo\\customer-support-rag\\data\\processed\\knowledge_base_v2.json\n",
      "‚úÖ Saved config: c:\\Users\\victo\\customer-support-rag\\data\\processed\\kb_config_v2.json\n"
     ]
    }
   ],
   "source": [
    "# ## 15. Save Knowledge Base\n",
    "\n",
    "# %%\n",
    "# Create processed data directory\n",
    "processed_dir = project_root / 'data' / 'processed'\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Saving knowledge base to: {processed_dir}\")\n",
    "\n",
    "# %%\n",
    "# Save as CSV (without embeddings - too large)\n",
    "csv_path = processed_dir / 'knowledge_base_v2.csv'\n",
    "kb_df_save = kb_df.drop(columns=['embedding', 'combined_text'])\n",
    "kb_df_save.to_csv(csv_path, index=False)\n",
    "print(f\"‚úÖ Saved CSV: {csv_path}\")\n",
    "\n",
    "# %%\n",
    "# Save as pickle (with embeddings)\n",
    "pickle_path = processed_dir / 'knowledge_base_v2_with_embeddings.pkl'\n",
    "kb_df.to_pickle(pickle_path)\n",
    "print(f\"‚úÖ Saved pickle: {pickle_path}\")\n",
    "\n",
    "# %%\n",
    "# Save as JSON (for portability)\n",
    "json_path = processed_dir / 'knowledge_base_v2.json'\n",
    "\n",
    "kb_export = []\n",
    "for idx, row in kb_df.iterrows():\n",
    "    entry = {\n",
    "        'id': f\"kb_{idx}\",\n",
    "        'question': row['text'],\n",
    "        'answer': row['answer'],\n",
    "        'category': row['category'],\n",
    "        'category_id': int(row['label']),\n",
    "        'word_count': int(row['word_count']),\n",
    "        'answer_length': int(row['answer_length']),\n",
    "        'has_negation': bool(row['has_negation']),\n",
    "        'is_complex': bool(row['is_complex']),\n",
    "        'question_type': row['question_type']\n",
    "    }\n",
    "    kb_export.append(entry)\n",
    "\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(kb_export, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved JSON: {json_path}\")\n",
    "\n",
    "# %%\n",
    "# Save enhanced configuration\n",
    "config_path = processed_dir / 'kb_config_v2.json'\n",
    "\n",
    "config = {\n",
    "    'version': '2.0',\n",
    "    'embedding_model': EMBEDDING_MODEL,\n",
    "    'answer_generation_model': 'gpt-4o-mini',\n",
    "    'total_entries': int(len(kb_df)),\n",
    "    'embedding_dimension': int(len(embeddings[0])),\n",
    "    'vector_db_path': str(vector_db_dir),\n",
    "    'collection_name': 'banking_support',\n",
    "    'categories': int(kb_df['category'].nunique()),\n",
    "    'created_date': datetime.now().isoformat(),\n",
    "    'is_sample': USE_SAMPLE,\n",
    "    'sample_size': int(SAMPLE_SIZE) if USE_SAMPLE else int(len(kb_df)),\n",
    "    'stratified_sampling': USE_STRATIFIED_SAMPLING if USE_SAMPLE else False,\n",
    "    'template_categories': list(ANSWER_TEMPLATES.keys()),\n",
    "    'template_usage_count': int(kb_df['category'].isin(ANSWER_TEMPLATES.keys()).sum()),\n",
    "    'statistics': {\n",
    "        'avg_query_length': float(kb_df['word_count'].mean()),\n",
    "        'avg_answer_length': float(kb_df['answer_length'].mean()),\n",
    "        'negation_queries': int(kb_df['has_negation'].sum()),\n",
    "        'complex_queries': int(kb_df['is_complex'].sum())\n",
    "    },\n",
    "    'evaluation_results': {\n",
    "        'real_evaluation': {\n",
    "            'test_set_size': len(real_eval_set),\n",
    "            'accuracy@1': float(real_results['accuracy@1']),\n",
    "            'accuracy@3': float(real_results['accuracy@3']),\n",
    "            'accuracy@5': float(real_results['accuracy@5']),\n",
    "            'category_accuracy': float(real_results['category_accuracy']),\n",
    "            'negation_accuracy': float(real_results['by_type']['negation']['category_accuracy']),\n",
    "            'complex_accuracy': float(real_results['by_type']['complex']['category_accuracy'])\n",
    "        },\n",
    "        'hybrid_search': {\n",
    "            'accuracy@1': float(hybrid_real_results['accuracy@1']),\n",
    "            'accuracy@3': float(hybrid_real_results['accuracy@3']),\n",
    "            'accuracy@5': float(hybrid_real_results['accuracy@5']),\n",
    "            'category_accuracy': float(hybrid_real_results['category_accuracy']),\n",
    "            'negation_accuracy': float(hybrid_real_results['by_type']['negation']['category_accuracy'])\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved config: {config_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9134333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä ENHANCED KNOWLEDGE BASE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìö Content:\n",
      "   Total entries: 10,003\n",
      "   Unique categories: 77\n",
      "   Avg question length: 11.9 words\n",
      "   Avg answer length: 60.7 words\n",
      "   Negation queries: 2184 (21.8%)\n",
      "   Complex queries: 1815 (18.1%)\n",
      "\n",
      "üî¢ Embeddings:\n",
      "   Model: text-embedding-3-small\n",
      "   Dimension: 1536\n",
      "   Total vectors: 10,003\n",
      "\n",
      "üíæ Storage:\n",
      "   Vector DB: c:\\Users\\victo\\customer-support-rag\\data\\vector_db\n",
      "   CSV: c:\\Users\\victo\\customer-support-rag\\data\\processed\\knowledge_base_v2.csv\n",
      "   Pickle: c:\\Users\\victo\\customer-support-rag\\data\\processed\\knowledge_base_v2_with_embeddings.pkl\n",
      "   JSON: c:\\Users\\victo\\customer-support-rag\\data\\processed\\knowledge_base_v2.json\n",
      "   Config: c:\\Users\\victo\\customer-support-rag\\data\\processed\\kb_config_v2.json\n",
      "   Real Eval Set: c:\\Users\\victo\\customer-support-rag\\data\\processed\\real_evaluation_set.json\n",
      "\n",
      "üìà Category Coverage:\n",
      "   Most common: card_payment_fee_charged (187 samples)\n",
      "   Least common: contactless_not_working (35 samples)\n",
      "   Average per category: 129.9\n",
      "\n",
      "üéØ Performance Metrics (on UNSEEN test data):\n",
      "   Accuracy@1: 89.0%\n",
      "   Accuracy@5: 96.0%\n",
      "   Negation Handling: 90.0%\n",
      "   Complex Queries: 85.0%\n",
      "\n",
      "‚úÖ EXCELLENT: High accuracy even with 10003 entries!\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Production-ready knowledge base created!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ## 16. Knowledge Base Summary\n",
    "\n",
    "# %%\n",
    "print(\"=\"*70)\n",
    "print(\"üìä ENHANCED KNOWLEDGE BASE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìö Content:\")\n",
    "print(f\"   Total entries: {len(kb_df):,}\")\n",
    "print(f\"   Unique categories: {kb_df['category'].nunique()}\")\n",
    "print(f\"   Avg question length: {kb_df['word_count'].mean():.1f} words\")\n",
    "print(f\"   Avg answer length: {kb_df['answer_length'].mean():.1f} words\")\n",
    "print(f\"   Negation queries: {kb_df['has_negation'].sum()} ({kb_df['has_negation'].sum()/len(kb_df)*100:.1f}%)\")\n",
    "print(f\"   Complex queries: {kb_df['is_complex'].sum()} ({kb_df['is_complex'].sum()/len(kb_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüî¢ Embeddings:\")\n",
    "print(f\"   Model: {EMBEDDING_MODEL}\")\n",
    "print(f\"   Dimension: {len(embeddings[0])}\")\n",
    "print(f\"   Total vectors: {len(embeddings):,}\")\n",
    "\n",
    "print(f\"\\nüíæ Storage:\")\n",
    "print(f\"   Vector DB: {vector_db_dir}\")\n",
    "print(f\"   CSV: {csv_path}\")\n",
    "print(f\"   Pickle: {pickle_path}\")\n",
    "print(f\"   JSON: {json_path}\")\n",
    "print(f\"   Config: {config_path}\")\n",
    "print(f\"   Real Eval Set: {real_eval_path}\")\n",
    "\n",
    "print(f\"\\nüìà Category Coverage:\")\n",
    "category_dist = kb_df['category'].value_counts()\n",
    "print(f\"   Most common: {category_dist.index[0]} ({category_dist.iloc[0]} samples)\")\n",
    "print(f\"   Least common: {category_dist.index[-1]} ({category_dist.iloc[-1]} samples)\")\n",
    "print(f\"   Average per category: {category_dist.mean():.1f}\")\n",
    "\n",
    "print(f\"\\nüéØ Performance Metrics (on UNSEEN test data):\")\n",
    "print(f\"   Accuracy@1: {real_results['accuracy@1']:.1%}\")\n",
    "print(f\"   Accuracy@5: {real_results['accuracy@5']:.1%}\")\n",
    "print(f\"   Negation Handling: {real_results['by_type']['negation']['category_accuracy']:.1%}\")\n",
    "print(f\"   Complex Queries: {real_results['by_type']['complex']['category_accuracy']:.1%}\")\n",
    "\n",
    "if real_results['accuracy@1'] < 0.5 and len(kb_df) < 1000:\n",
    "    print(f\"\\n‚ö†Ô∏è  RECOMMENDATION: Performance limited by small KB size ({len(kb_df)} entries)\")\n",
    "    print(f\"   Run full production build for 70-85% accuracy\")\n",
    "elif real_results['accuracy@1'] >= 0.7:\n",
    "    print(f\"\\n‚úÖ EXCELLENT: High accuracy even with {len(kb_df)} entries!\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ GOOD: Solid baseline performance\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    print(\"‚ö†Ô∏è  NOTE: This is a SAMPLE knowledge base for testing\")\n",
    "    print(\"Set USE_SAMPLE = False to create production KB with all 10,003 queries\")\n",
    "else:\n",
    "    print(\"‚úÖ Production-ready knowledge base created!\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83cc12d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç FINAL VERIFICATION\n",
      "======================================================================\n",
      "‚úÖ Answers generated\n",
      "‚úÖ Embeddings created\n",
      "‚úÖ No missing values\n",
      "‚úÖ Negation flags set\n",
      "‚úÖ Vector DB populated\n",
      "‚úÖ Vector DB count matches\n",
      "‚úÖ CSV exported\n",
      "‚úÖ Pickle exported\n",
      "‚úÖ JSON exported\n",
      "‚úÖ Config saved\n",
      "‚úÖ Real eval set saved\n",
      "‚úÖ Basic retrieval works\n",
      "‚úÖ Hybrid retrieval works\n",
      "‚úÖ Real evaluation completed\n",
      "‚úÖ Test data loaded\n",
      "‚úÖ Advanced metrics calculated\n",
      "\n",
      "======================================================================\n",
      "üéâ All checks passed! Enhanced knowledge base is ready!\n",
      "\n",
      "üìã Next Steps:\n",
      "1. Review performance metrics above\n",
      "2. ‚úÖ Production KB complete - ready for deployment\n",
      "3. Build RAG retrieval + generation pipeline\n",
      "4. Create Streamlit chatbot interface\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ## 17. Final Verification\n",
    "\n",
    "# %%\n",
    "print(\"\\nüîç FINAL VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "checks = []\n",
    "\n",
    "# Data checks\n",
    "checks.append((\"Answers generated\", 'answer' in kb_df.columns and kb_df['answer'].notna().all()))\n",
    "checks.append((\"Embeddings created\", 'embedding' in kb_df.columns))\n",
    "checks.append((\"No missing values\", kb_df[['text', 'answer', 'category']].notna().all().all()))\n",
    "checks.append((\"Negation flags set\", 'has_negation' in kb_df.columns))\n",
    "\n",
    "# Vector DB checks\n",
    "try:\n",
    "    count = collection.count()\n",
    "    checks.append((\"Vector DB populated\", count > 0))\n",
    "    checks.append((\"Vector DB count matches\", count == len(kb_df)))\n",
    "except:\n",
    "    checks.append((\"Vector DB populated\", False))\n",
    "    checks.append((\"Vector DB count matches\", False))\n",
    "\n",
    "# File checks\n",
    "checks.append((\"CSV exported\", csv_path.exists()))\n",
    "checks.append((\"Pickle exported\", pickle_path.exists()))\n",
    "checks.append((\"JSON exported\", json_path.exists()))\n",
    "checks.append((\"Config saved\", config_path.exists()))\n",
    "checks.append((\"Real eval set saved\", real_eval_path.exists()))\n",
    "\n",
    "# Functionality checks\n",
    "try:\n",
    "    test_results = search_knowledge_base(\"test query\", n_results=1)\n",
    "    checks.append((\"Basic retrieval works\", len(test_results['documents'][0]) > 0))\n",
    "except:\n",
    "    checks.append((\"Basic retrieval works\", False))\n",
    "\n",
    "if USE_HYBRID_SEARCH and BM25_AVAILABLE:\n",
    "    try:\n",
    "        test_hybrid = hybrid_search_smart(\"test query\", n_results=1)\n",
    "        checks.append((\"Hybrid retrieval works\", len(test_hybrid['documents'][0]) > 0))\n",
    "    except:\n",
    "        checks.append((\"Hybrid retrieval works\", False))\n",
    "\n",
    "# Performance checks\n",
    "checks.append((\"Real evaluation completed\", len(real_eval_set) > 0))\n",
    "checks.append((\"Test data loaded\", len(test_df) > 0))\n",
    "checks.append((\"Advanced metrics calculated\", 'mrr' in advanced_metrics))\n",
    "\n",
    "# Print results\n",
    "for check_name, passed in checks:\n",
    "    status = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "    print(f\"{status} {check_name}\")\n",
    "\n",
    "all_passed = all(passed for _, passed in checks)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "if all_passed:\n",
    "    print(\"üéâ All checks passed! Enhanced knowledge base is ready!\")\n",
    "    print(\"\\nüìã Next Steps:\")\n",
    "    print(\"1. Review performance metrics above\")\n",
    "    if USE_SAMPLE:\n",
    "        print(\"2. üöÄ IMPORTANT: Set USE_SAMPLE = False and run full production build\")\n",
    "        print(\"   Expected improvement: 62% ‚Üí 75-85% accuracy\")\n",
    "    else:\n",
    "        print(\"2. ‚úÖ Production KB complete - ready for deployment\")\n",
    "    print(\"3. Build RAG retrieval + generation pipeline\")\n",
    "    print(\"4. Create Streamlit chatbot interface\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Some checks failed. Please review above.\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
